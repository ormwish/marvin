{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Marvin \ud83e\udd16\ud83c\udfd6\ufe0f","text":"<p>Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function. Marvin is building the gears, cranks, and pistons to let mechanical software systems integrate with and leverage the raw power of Large Language Models.</p> <p>Marvin introduces two new concepts:</p> <ul> <li> <p>AI Models. These models, grounded in Pydantic, offer a transformative approach to data processing by converting unstructured contexts into type-safe outputs that validate against your model schema. This empowers you to interrogate your data through your schema. AI models present an elegant solution that combines the potent reasoning capabilities of AI with the sturdy, type-safe boundaries set by Pydantic.</p> </li> <li> <p>AI Functions. These functions differ from conventional ones in that they don\u2019t rely on source code, but instead generate their outputs on-demand with AI by using an LLM as a runtime. With AI functions, you don't have to write complex code for tasks like extracting entities from web pages, scoring sentiment, or categorizing items in your database. Just describe your needs, call the function, and you're done!</p> </li> </ul> <p>AI functions and models work with native data types, so you can seamlessly integrate them into any codebase and chain them into sophisticated pipelines. Technically speaking, Marvin transforms the signature of using AI from <code>(str) -&gt; str</code> to <code>(**kwargs) -&gt; Any</code>. </p> <p>Marvin leverages \"functional prompt engineering\" to build a battle-tested and typesafe interface to LLMs. We expose our API through drop-in decorators, creating an unparalleled developer experience so that every developer can reach for LLMs when it's the right tool. </p> <p>Marvin is powered by a fleet of powerful, flexible, and secure bots. Bots are highly capable AI assistants that can be given specific instructions and personalities or roles. They can use custom plugins and leverage external knowledge, and automatically create a history of every thread. Under the hood, AI functions are actually a type of bot. </p> <p>To make it easy to work with bots, Marvin includes a fully-functional TUI. The TUI tracks threads across multiple bots and even lets you manage your bots through a conversational interface.</p> <p>Developers can use\u00a0Marvin to add AI capabilities wherever they will be most impactful, without needing to start from scratch. Marvin's code is available on GitHub, and say hello on our Discord server!</p> <p>Marvin is built with \ud83d\udc99 by Prefect.</p>"},{"location":"#features","title":"Features","text":"<p>\u2699\ufe0f Create AI Models to make data models that can derive structured information from unstructured text data</p> <p>\ud83e\ude84 Write AI functions to process structured data without source code or generate high quality synthetic data</p> <p>\ud83e\udd16 Build bots that have personalities and follow instructions</p> <p>\ud83d\udda5\ufe0f Chat with bots in a fully-featured TUI</p> <p>\ud83d\udd0c Give your bots new abilities with plugins </p> <p>\ud83d\udcda Store knowledge that bots can access and use</p> <p>\ud83d\udce1 Available as a Python API, interactive CLI, or FastAPI server</p>"},{"location":"#quick-start","title":"Quick start","text":"<ol> <li>Install: <code>pip install marvin</code></li> <li>Chat: <code>marvin chat</code></li> </ol>"},{"location":"#slightly-less-quick-start","title":"Slightly less quick start","text":"<p>Create a bot: <pre><code>marvin bots create ObiWanKenoBot -p \"knows every Star Wars meme\"\n</code></pre> Chat with it: <pre><code>marvin chat -b ObiWanKenoBot\n</code></pre> </p> <p>See the getting started docs for more.</p>"},{"location":"#open-source","title":"Open source","text":"<p>Marvin is open-source with an Apache 2.0 license and built on standards like Pydantic, FastAPI, Langchain, and Prefect. The code is available on GitHub.</p> <p>Construction zone</p> <p>Marvin is under active development and is likely to change. </p>"},{"location":"#coming-soon","title":"Coming soon","text":"<p>\u267b\ufe0f Interactive AI functions</p> <p>\ud83d\uddbc\ufe0f Admin and chat UIs</p> <p>\ud83c\udfd7\ufe0f Advanced data loading and preprocessing</p> <p>\ud83d\udd2d AI orchestration and observability platform</p> <p>\ud83d\ude80 Quick deploys of LLM-powered APIs</p> <p>\ud83c\udf81 Quickstarts for common use cases</p>"},{"location":"#when-should-you-use-marvin","title":"When should you use Marvin?","text":"<p>Marvin is an opinionated, high-level library with the goal of integrating AI tools into software development. There are a few major reasons to use Marvin:</p> <ol> <li> <p>You want an AI function that can process structured data. Marvin brings the power of AI to native data structures, letting you build functions that would otheriwse be difficult or even impossible to write. For example, you can use AI functions to make a list of all the animals in a paragraph, generate JSON documents from HTML content, extract keywords that match some criteria, or categorize sentiment -- without any traditional source code.</p> </li> <li> <p>You want an AI assistant in your code. Marvin's bots can follow instructions and hold conversations to solve complex problems. They can use custom plugins and take advantage of external knowledge. They are designed to be integrated into your codebase, but of course you can expose them directly to your users as well!</p> </li> <li> <p>You want to deploy cutting-edge AI technology with confidence, but without having to make too many decisions. Using LLMs successfully requires very careful consideration of prompts, data preprocessing, and infrastructure. Our target user is more interested in using AI systems than building AI systems. Therefore, Marvin is designed to make adopting this technology as straightforward as possible by optimizing for useful outcomes. Marvin's prompts have been hardened by months of real-world use and will continue to improve over time.</p> </li> </ol>"},{"location":"#when-should-you-not-use-marvin","title":"When should you NOT use Marvin?","text":"<p>There are a few reasons NOT to use Marvin:</p> <ol> <li> <p>You want full control of an AI. Marvin is a high-level library and (with few exceptions) does not generally expose LLM configuration to users. We have chosen settings that give the best results under most circumstances, taking Marvin's built-in prompts into consideration.</p> </li> <li> <p>You want an AI copilot for writing code. Marvin's job isn't to help you write source code; it's to help you do things that are difficult or impossible to express in source code. That could range from mundane activities to writing a function that can extract the names of animals commonly found in North America from an email (yes, it's a ridiculous example - but it's possible). Modern LLMs excel at complex reasoning, and Marvin lets you bring that into your code in a way that feels native and natural.</p> </li> <li> <p>You want full control of your prompts. As a \"functional prompt engineering\" platform, Marvin takes user inputs and generates prompts that are likely to deliver the outcome the user wants, even if they are not verbatim what the user said. Marvin does not expect users to send completely raw prompts to the LLM. </p> </li> <li> <p>You're searching for the Ultimate Question. While Marvin is highly intelligent, even he couldn't come up with the Ultimate Question of Life, the Universe, and Everything. If you're seeking existential enlightenment, you might need to look beyond our beloved paranoid android.</p> </li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#how-do-i-report-a-problem","title":"How do I report a problem?","text":"<p>Marvin is under rapid development and has a few sharp edges! If you run into trouble, please open an issue here.</p>"},{"location":"faq/#should-i-use-gpt-4-or-gpt-35","title":"Should I use GPT-4 or GPT-3.5?","text":"<p>Marvin supports multiple LLM models. At this time, models include OpenAI's GPT-4 (<code>gpt-4</code>) and GPT-3.5 (<code>gpt-3.5-turbo</code>). To set the model, use the environment variable <code>MARVIN_LLM_MODEL</code>. Because not every developer has access to GPT-4 (yet), Marvin's default model is GPT-3.5. This guarantees that everyone can use Marvin \"out of the box.\"</p> <p>Performance is much better on GPT-4 than GPT-3.5, though GPT-3.5 is still very good for many use cases. In particular, GPT-4 is better at following instructions over long interactions and staying \"on-script\" throughout an entire conversation. It is much less susceptible to being distracted and can break problems down into manageable pieces more easily. However, it is slower and up to 30x more expensive than GPT-3.5, and is also not yet widely available to all OpenAI accounts. Many of Marvin's prompts were originally written for GPT-3.5, which is one of the reasons the smaller model still has great results. In our experience, prompts optimized for GPT-4 usually fail outright with GPT-3.5.</p>"},{"location":"faq/#python-api","title":"Python API","text":""},{"location":"faq/#how-do-i-run-async-code","title":"How do I run async code?","text":"<p>Marvin is an async library because the vast majority of time is spent waiting for LLM responses to be returned via API. Therefore, it can be used natively in any other async library. </p> <p>The standard Python repl doesn't allow you to directly <code>await</code> async coroutines, but interpreters like IPython do (IPython is included as a Marvin development dependency).</p> <p>To integrate bots into synchronous frameworks, wrap async calls in <code>asyncio.run(coro)</code> or use convenience methods like <code>Bot.say_sync()</code>. Marvin uses a library called <code>nest-asyncio</code> to run nested event loops in a way that Python doesn't usually permit.</p>"},{"location":"faq/#marvin","title":"Marvin","text":""},{"location":"faq/#who-maintains-marvin","title":"Who maintains Marvin?","text":"<p>Marvin is built with \ud83d\udc99 by Prefect.</p>"},{"location":"faq/#is-marvin-open-source","title":"Is Marvin open-source?","text":"<p>Marvin is fully open-source under an Apache 2.0 license.</p>"},{"location":"faq/#where-is-marvins-code","title":"Where is Marvin's code?","text":"<p>Marvin's code can be found on GitHub.</p>"},{"location":"faq/#why-marvin","title":"Why \"Marvin\"?","text":"<pre><code>from marvin import Bot\nbot = Bot()\nresponse = await bot.say(\"Why are you called Marvin?\")\nprint(response.content)\n# Ah, a question of origins! The name \"Marvin\" might be inspired by the\n# character Marvin the Paranoid Android from Douglas Adams' \"The Hitchhiker's\n# Guide to the Galaxy\" series. Marvin the Paranoid Android was an artificially\n# intelligent character with a distinct personality. However, my purpose here is\n# to be a clever, fun, and helpful assistant for you. I hope I can bring a\n# smile to your face while assisting you with your questions and tasks!\n</code></pre>"},{"location":"faq/#what-is-marvin","title":"What... is Marvin?","text":"<p>The first time we released an early version of Marvin in the Prefect Slack, the quality and tone of the answers was so good that some of our team became convinced that the demo was staged, with our CTO operating the Marvin account like a sock puppet. </p> <p>The idea stuck.</p>"},{"location":"faq/#whoa-code","title":"Whoa code","text":"<p>AI functions use LLMs as a runtime and don't need any source code. We call that whoa-code.</p>"},{"location":"development/development/","title":"Development","text":""},{"location":"development/development/#installing-for-development","title":"Installing for development","text":"<p>For development, you should clone the git repo and create an editable install including all development dependencies. To do so, run the following:</p> <pre><code>git clone https://github.com/prefecthq/marvin.git\ncd marvin\npip install -e \".[dev]\"\n</code></pre> <p>Please note that editable installs require <code>pip &gt;= 21.3</code>. To check your version of pip, run <code>pip --version</code> and, if necessary, <code>pip install -U pip</code> to upgrade.</p>"},{"location":"development/development/#static-analysis","title":"Static analysis","text":"<p>In order to merge a PR, code must pass a static analysis check. Marvin uses <code>ruff</code> and <code>black</code> to ensure consistently formatted code. At this time, we do not do a static typing check, but we encourage full type annotation. </p> <p>To run the checks locally, you can use pre-commit.</p> <p>Pre-commit is included as a development dependency. To set it up, run the following from your <code>marvin</code> root directory:</p> <pre><code>pre-commit install\n</code></pre> <p>The pre-commit checks will now be run on every commit you make. You can run them yourself with:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"development/development/#unit-tests","title":"Unit tests","text":"<p>Marvin's unit tests live in the <code>tests/</code> directory. There are two types of unit tests; those that require LLM calls and those that don't. Tests that require LLM calls should be put in the <code>tests/llm_tests</code> directory. They are run separately because they have different time and cost constraints than standard tests.</p> <p>Marvin uses pytest to run tests. To invoke it: <pre><code># run all tests\npytest\n\n# run only LLM tests\npytest -m \"llm\"\n# run only non-LLM tests\npytest -m \"not llm\"\n</code></pre></p>"},{"location":"getting_started/ai_functions_quickstart/","title":"AI Functions","text":"<p>AI functions are functions that are defined locally but use AI to generate their outputs. Like normal functions, AI functions take arguments and return structured outputs like <code>lists</code>, <code>dicts</code> or even Pydantic models. Unlike normal functions, they don't need any source code! </p> <p>Consider the following example, which contains a function that generates a list of fruits. The function is defined with a descriptive name, annotated input and return types, and a docstring -- but doesn't appear to actually do anything. Nonetheless, because of the <code>@ai_fn</code> decorator, it can be called like a normal function and returns a list of fruits.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(n=3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre> <p>AI functions are especially useful for activies that would be difficult, time-consuming, or impossible to code. They are particularly powerful for parsing and processing strings, but can be used with almost any data structure. Here are a few more examples:</p> <p><pre><code>@ai_fn\ndef extract_animals(text: str) -&gt; list[str]:\n\"\"\"Returns a list of all animals mentioned in the text\"\"\"\n</code></pre> <pre><code>@ai_fn\ndef classify_sentiment(tweets: list[str]) -&gt; list[bool]:\n\"\"\"\n    Given a list of tweets, classifies each one as \n    positive (true) or negative (false) and returns \n    a corresponding list\n    \"\"\"\n</code></pre> <pre><code>@ai_fn\ndef suggest_title(article: str, style: str=None) -&gt; str:\n\"\"\"\n    Suggest a title for the provided article, optionally in \n    the style of a publication (such as the AP, NYTimes, etc.)\n    \"\"\"\n</code></pre> <pre><code>@ai_fn\ndef extract_keywords(text:str, criteria:str=None) -&gt; list[str]:\n\"\"\"\n    Extract important keywords from text, optionally only including \n    those that meet the provided criteria (for example, \"colors\", \n    \"proper nouns\", or \"European capitals\")\n    \"\"\"\n</code></pre></p> <p>For more information about AI functions, including examples and how to include executable code in your function, see the AI function docs.</p>"},{"location":"getting_started/ai_models_quickstart/","title":"AI Functions","text":"<p>AI models, grounded in Pydantic, offer a transformative approach to data processing by converting unstructured contexts into type-safe outputs that align with your model schema. This empowers you to interrogate your data through your schema, by combining the potent reasoning capabilities of AI with the type boundaries set by Pydantic.</p> <p>The challenge of transforming unstructured text data into a structured format is a familiar adversary to engineers, analysts, and data scientists. Traditionally, dealing with unstructured data has been the exclusive domain of specialists, requiring the creation of custom models for each data feature. Overlooked a feature? Time to reset the clock for the next quarter. However, with AI models, adjusting your schema suffices, and Marvin handles the rest.</p> <p>AI Models make inference declarative - they offer a method to generate synthetic and hyper-realistic training data for custom tools.</p> <p>Let's illustrate with a practical example. Suppose we aim to devise a system to parse resumes in an applicant tracking system. Resumes are diverse in shape, size, and format. In the past, data scientists would craft an array of regular expressions and custom Natural Language Processing (NLP) models to extract entities such as companies or universities and link them to specific dates. The introduction of each new feature would trigger a fresh development cycle.</p> <p>With Marvin, you employ Pydantic to shape your data model as per usual and enhance your model with <code>@ai_model</code>. This imparts an extraordinary capability to your Pydantic model: the capacity to manage unstructured text.</p> <pre><code>from marvin import ai_model\nimport pydantic\nfrom typing import Optional\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\nResume('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io').json(indent = 2)\n#{\n#     first_name: 'Ford',\n#     last_name: 'Prefect',\n#     email: 'ford@prefect.io',\n#     phone: '(555) 5124-5242',\n#}\n</code></pre> <p>This is a rather idealized scenario, so let's delve into a more realistic use case. Imagine a real-world situation where resumes are not as neat and predictable. They may include varied sections like work history, education, skills, and they might even contain unconventional structures or typos. Let's enrich our model to handle this complexity:</p> <pre><code>import datetime\nfrom typing import List, Literal, Optional, Union\nimport pydantic\nfrom marvin import ai_model\nclass Institution(pydantic.BaseModel):\nname: str\nstart_date: Optional[datetime.date]\nend_date: Union[datetime.date, Literal['Present']]\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\neducation: List[Institution]\nwork_experience: List[Institution]\nResume(\"\"\"\nFord Prefect\nContact: (555) 5124-5242, ford@prefect.io\nEducation:\n- University of Betelgeuse, 1965 - 1969\n- School of Galactic Travel, 1961 - 1965\nWork Experience:\n- The Hitchhiker's Guide to the Galaxy, Researcher, 1979 - Present\n- Galactic Freelancer, 1969 - 1979\\\n\"\"\").json(indent = 2)\n# {\n#   \"first_name\": \"Ford\",\n#   \"last_name\": \"Prefect\",\n#   \"phone_number\": \"(555) 5124-5242\",\n#   \"email\": \"ford@prefect.io\",\n#   \"education\": [\n#     {\n#       \"name\": \"University of Betelgeuse\",\n#       \"start_date\": \"1965-01-01\",\n#       \"end_date\": \"1969-01-01\"\n#     },\n#     {\n#       \"name\": \"School of Galactic Travel\",\n#       \"start_date\": \"1961-01-01\",\n#       \"end_date\": \"1965-01-01\"\n#     }\n#   ],\n#   \"work_experience\": [\n#     {\n#       \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n#       \"start_date\": \"1979-01-01\",\n#       \"end_date\": \"Present\"\n#     },\n#     {\n#       \"name\": \"Galactic Freelancer\",\n#       \"start_date\": \"1969-01-01\",\n#       \"end_date\": \"1979-01-01\"\n#     }\n#   ]\n# }\n</code></pre> <p>The magic here is not just that Marvin can parse this structure, but that it can also evolve with the schema. If we decide tomorrow that we want to parse out a new section, or get more granular with the dates, we just change our Pydantic schema, and Marvin takes care of the rest.</p> <p>One of the most powerful features of AI Models is that it can infer and/or derive information.  Let's revist the previous example, but let's say we want our Experience model to infer derived information.</p> <pre><code>from marvin import ai_model\nimport pydantic\nfrom typing import List, Literal, Optional\nclass Institution(pydantic.BaseModel):\nname: str\nstart_date: Optional[datetime.date]\nend_date: Union[datetime.date, Literal['Present']]\nclass Technology(pydantic.BaseModel):\ntechnology: str = pydantic.Field(description = 'e.g. SQL, Python')\nyears_of_experience: int\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\neducation: List[Institution]\nwork_experience: List[Institution]\nyears_of_experience: int\ntechnologies: List[Technology]\nResume(\"\"\"\\\nFord Prefect\nContact: (555) 5124-5242, ford@prefect.io\nEducation:\n- University of Betelgeuse, 1965 - 1969\n- School of Galactic Travel, 1961 - 1965\nWork Experience:\n- The Hitchhiker's Guide to the Galaxy, Researcher, 1979 - Present\n    \u2022 Proficient in data analysis and database management, utilizing tools such as Excel and SQL to maintain a comprehensive interstellar knowledge base.\n    \u2022 Skilled in interstellar travel logistics, including navigation, transportation, and accommodation arrangements, ensuring smooth and efficient interstellar expeditions.\n    \u2022 Experienced in multimedia production, utilizing software such as Adobe Creative Suite to create engaging and informative content for various mediums, including video, audio, and graphic design.\n- Galactic Freelancer, 1969 - 1979\\\n\"\"\")\n# {\n#   \"first_name\": \"Ford\",\n#   \"last_name\": \"Prefect\",\n#   \"phone_number\": \"(555) 5124-5242\",\n#   \"email\": \"ford@prefect.io\",\n#   \"education\": [\n#     {\n#       \"name\": \"University of Betelgeuse\",\n#       \"start_date\": \"1965-01-01\",\n#       \"end_date\": \"1969-01-01\"\n#     },\n#     {\n#       \"name\": \"School of Galactic Travel\",\n#       \"start_date\": \"1961-01-01\",\n#       \"end_date\": \"1965-01-01\"\n#     }\n#   ],\n#   \"work_experience\": [\n#     {\n#       \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n#       \"start_date\": \"1979-01-01\",\n#       \"end_date\": \"Present\"\n#     },\n#     {\n#       \"name\": \"Galactic Freelancer\",\n#       \"start_date\": \"1969-01-01\",\n#       \"end_date\": \"1979-01-01\"\n#     }\n#   ],\n#   \"years_of_experience\": 54,\n#   \"technologies\": [\n#     {\n#       \"technology\": \"Excel\",\n#       \"years_of_experience\": 42\n#     },\n#     {\n#       \"technology\": \"SQL\",\n#       \"years_of_experience\": 42\n#     },\n#     {\n#       \"technology\": \"Adobe Creative Suite\",\n#       \"years_of_experience\": 42\n#     }\n#   ]\n# }\n</code></pre> <p>AI Models are not just a tool; they're a revolution in how we handle unstructured data, making our lives easier and our data richer. They open up new avenues for creativity and productivity, effectively blurring the lines between AI and conventional software systems. With AI Models, we are truly pushing the boundaries of what's possible.</p> <p>For more information about AI Models, including examples, see the AI Models docs.</p>"},{"location":"getting_started/bots_quickstart/","title":"Bots","text":"<p>Bots are AI assistants that can take instructions over multiple interactions. </p> <p>To create a new interactive bot, instantiate the <code>Bot</code> class with instructions, a personality, or plugins. You can begin talking to it with the <code>say()</code> method. Bots have a memory, so if you call <code>say()</code> again, the bot will recall your conversation.</p> <p>Note</p> <p>Marvin is an async library and the <code>say()</code> method must be awaited. Bots also have a synchronous <code>say_sync()</code> method for convenience.</p> <pre><code>from marvin import Bot\nbot = Bot(personality='knows every Star Wars meme')\nawait bot.say('Hello there')\nawait bot.say('How do you feel about sand?')\n</code></pre> <p>By combining personalities, instructions, and plugins, you can get bots to solve complex problems that would be difficult to address in traditional code. Bots can also be exposed directly to users to act as assistants or interactive guides.</p> <p>For more information about bots, see the bot docs.</p>"},{"location":"getting_started/chat_quickstart/","title":"Chat","text":""},{"location":"getting_started/chat_quickstart/#quick-chat","title":"Quick chat","text":"<p>To quickly jump into a chat, run <code>marvin chat</code> from your command line. This will open a new session with the default chatbot, whose name is Marvin. You can type messages to the bot, and it will respond. </p> <pre><code>marvin chat\n</code></pre> <p>To learn more about the Marvin TUI, see its documentation</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"getting_started/installation/#for-normal-use","title":"For normal use","text":"<p>To install Marvin, run <code>pip install marvin</code>. As a matter of best practice, we recommend installing Marvin in a virtual environment.</p> <p>To use Marvin's knowledge features, please include the Chroma dependency: <code>pip install \"marvin[chromadb]\"</code>.</p> <p>Marvin uses OpenAI's GPT-3.5 (ChatGPT) model by default. Before using Marvin, you'll need to provide an OpenAI API key (via the the <code>MARVIN_OPENAI_API_KEY</code> or <code>OPENAI_API_KEY</code> environment variables) or choose a different LLM backend.</p>"},{"location":"getting_started/installation/#for-development","title":"For development","text":"<p>To install Marvin for development, please see the development guide.</p>"},{"location":"guide/ai_functions/data/","title":"AI Functions for data","text":"<p>\"Data cleaning is 80% of data science.\"</p> <p>-- Everyone</p> <p>Data cleaning is an essential step in the data analysis pipeline. Ensuring that your data is in the right format and free of inconsistencies, errors, or missing values can greatly improve the quality of your results and help you draw accurate conclusions. While some aspects of data cleaning are straightforward and can be addressed through methods that are straightforward to express in code, such as replacing null values with 0, removing missing data, trimming outliers, or changing capitalization, there are other aspects that are more complex and harder to handle programmatically.</p> <p>That's where AI functions come in. These powerful AI-enabled data tools can tackle complex data cleaning tasks that are difficult to automate using traditional techniques. Examples include converting values into less-granular categories, filling missing data based on context, extracting entities, and standardizing responses. In this document, we will briefly overview the LLM-powered data cleaning functions and demonstrate how they can be utilized to enhance your data cleaning process.</p>"},{"location":"guide/ai_functions/data/#ai-functions-for-data_1","title":"AI functions for data","text":"<p>AI functions provide a sophisticated and efficient solution for handling complex data-related tasks. These functions offer a distinct advantage over traditional programmatic methods, as they are equipped to manage tasks that are difficult or impossible to achieve using conventional techniques.</p> <p>One of the primary benefits of AI functions for data is their ability to understand context. In many cases, the context surrounding a piece of data is crucial for correctly transforming or filling in missing values. For example, when filling in missing state information for cities, an AI function can leverage its vast knowledge of geographical information to accurately determine the correct state. This context-awareness enables AI functions for data to handle a wide variety of tasks that would otherwise require significant manual effort or intricate, custom-built algorithms.</p> <p>Another key advantage of AI functions for data is their capacity to handle less structured data or data that involves some level of human interpretation. For instance, when data needs to be categorized or mapped to a specific set of values, AI functions can utilize their understanding of language and concepts to accurately determine the appropriate category for each data point. This is particularly useful when dealing with survey responses or other forms of textual data that may not adhere to a standardized format. Importantly, AIs can map granular data to either a known set of categories or determine the appropriate categories on their own. </p> <p>AI functions for data also excel at entity extraction, a process that involves pulling structured data from unstructured text. Examples of entity extraction include converting items on a resume to JSON or extracting zip codes or countries from addresses. By leveraging their language understanding capabilities, AI functions can effectively parse unstructured text and identify key pieces of information, making it easier to analyze, manipulate, and integrate this data into other systems.</p> <p>In addition, AI functions for data can standardize data in various formats, such as phone numbers, dates, addresses, or names. This allows for more consistent data representation and improves compatibility between different datasets. These functions can automatically detect various formats and convert them into a unified standard, saving time and reducing the risk of errors that can arise from manual data standardization.</p>"},{"location":"guide/ai_functions/data/#marvins-data-functions","title":"Marvin's data functions","text":""},{"location":"guide/ai_functions/data/#categorization","title":"Categorization","text":"<p>Marvin has two AI functions for categorization. <code>map_categories</code> maps a list of data to a set of known categories, while <code>categorize</code> maps a list of data to a set of categories defined by a natural language description.</p> <p>For example, the following are essentially equivalent ways of converting a list of color names to its nearest \"primary\" color:</p> <pre><code>from marvin.ai_functions.data import map_categories, categorize\ncolors_data = [\n\"teal\", \n\"cyan\",\n\"peach\",\n\"salmon\",\n\"red-orange\",\n\"lime\",\n...\n]\n# assign categories from a known list\nmap_categories(colors_data, categories=['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet'])\n# describe possible categories in natural language\ncategorize(colors_data, description='the colors of the rainbow')\n</code></pre>"},{"location":"guide/ai_functions/data/#standardization","title":"Standardization","text":"<p>Marvin has a <code>standardize</code> function that takes a list of data and converts it to a standard described in natural language. For example:</p> <pre><code>from marvin.ai_functions.data import standardize\nsurvey_data=[\n\"(555) 555 5555\", \n\"555-555-5555\", \n\"5555555555\", \n\"555.555.5555\"\n]\n# converts all values to (555) 555-5555\nstandardize(survey_data, format=\"US phone number with area code\")\n</code></pre>"},{"location":"guide/ai_functions/data/#context-aware-fill-for-missing-values","title":"Context-aware fill for missing values","text":"<p>Dealing with missing values is always difficult. There are many techniques for dealing with missing values, including removing them, filling them with a known value, or using analytical methods that properly account for them. AI functions introduce a new option: filling them based on context. Note that this approach can only be used when there are strong correlations in the dataset; otherwise, the context is not useful for predicting missing values. This is also a form of data augmentation and will not be appropriate for all use cases.</p> <p>Marvin has two functions for context-aware fill that differ only in input and return types. <code>context_aware_fillna</code> takes a matrix of data (organized as list[list] such that each inner list is a row of data) and column names; <code>context_aware_fillna_df</code> takes and returns Pandas dataframes.</p> <p>A major use case for context-aware fill is not arbitrarily inventing missing data (which has issues of its own) but transforming freeform responses into structured data. Survey responses are a good example of this. Consider a question that asks where the user lives. Responses could range from \"NYC\" to \"New York City, New York\" to \"BOS\". Extracting a city and state from these answers is a considerable NLP challenge. We can use a context aware fill to generate two empty (\"missing\") columns for city and state, then fill them:</p> <pre><code>import pandas as pd\nfrom marvin.ai_functions.data import context_aware_fillna_df\nsurvey_responses = pd.DataFrame(\n[\n[\"NY, NY\", None, None],\n[\"Boston, Massachusetts\", None, None],\n[\"Boston MA\", None, None],\n[\"NYC\",  None, None],\n],\ncolumns=[\"response\", \"city\", \"state\"],\n)\ncontext_aware_fillna_df(survey_responses)\n# Result:\n#                 response      city          state\n# 0                 NY, NY  New York       New York\n# 1  Boston, Massachusetts    Boston  Massachusetts\n# 2              Boston MA    Boston  Massachusetts\n# 3                    NYC  New York       New York\n</code></pre> <p>We can use a similar technique to augment existing data with AI-generated attributes:</p> <pre><code>import pandas as pd\nfrom marvin.ai_functions.data import context_aware_fillna_df\ncities_data = pd.DataFrame(\n[\n[\"New York\", None],\n[\"Massachusetts\", None],\n[\"California\", None],\n[\"Florida\",  None],\n],\ncolumns=[\"state\", \"capital\"],\n)\ncontext_aware_fillna_df(cities_data)\n# Result:\n#            state      capital\n# 0       New York       Albany\n# 1  Massachusetts       Boston\n# 2     California   Sacramento\n# 3        Florida  Tallahassee\n</code></pre> <p>Finally, this highly-stylized example shows the LLM's ability to fill in appropriate structure and datatypes all at once:</p> <pre><code>import pandas as pd\nfrom marvin.ai_functions.data import context_aware_fillna_df\nmovies_data = pd.DataFrame(\n[\n[\"The Terminator\", 1984, None],\n[\"Minority Report\", None, \"Steven Spielberg\"],\n[\"WALL-E\", None, \"Andrew Stanton\"],\n[\"Blade Runner\", 1982, None],\n],\ncolumns=[\"title\", \"release_year\", \"director\"],\n)\ncontext_aware_fillna_df(movies_data)\n# Result:\n#\n#                 title  release_year          director\n#    0   The Terminator          1984     James Cameron\n#    1  Minority Report          2002  Steven Spielberg\n#    2           WALL-E          2008    Andrew Stanton\n#    3     Blade Runner          1982      Ridley Scott\n</code></pre>"},{"location":"guide/ai_functions/data/#actual-title-case","title":"Actual title case","text":"<p>Return a title case string that you would want to use in a title.</p> <p>The Python string method <code>.title()</code> makes the first letter of every word uppercase and the remaing letters lowercase. This result isn't what you want to use for the title of a piece of writing, generally. <code>title_case</code> takes a string and returns a string you can use in a title.</p> <pre><code>from marvin.ai_functions.strings import title_case\ntitle_case(\"the european went over to canada, eh?\")\n# The European Went Over to Canada, Eh?\n</code></pre>"},{"location":"guide/ai_functions/entities/","title":"AI Functions for entities","text":"<p>AI functions are powerful tools for extracting structured data from unstructured text.</p>"},{"location":"guide/ai_functions/entities/#extract-keywords","title":"Extract keywords","text":"<pre><code>from marvin.ai_functions.entities import extract_keywords\ntext = (\n'The United States passed a law that requires all cars to have a \"black'\n' box\" that records data about the car and its driver. The law is sponsored'\n\" by John Smith. It goes into effect in 2025.\"\n)\nextract_keywords(text)\n# [\"United States\", \"law\", \"cars\", \"black box\", \"records\", \"data\", \"driver\", \"John Smith\", \"2025\"]\n</code></pre>"},{"location":"guide/ai_functions/entities/#extract-named-entitites","title":"Extract named entitites","text":"<p>This function extracts named entities, tagging them with spaCy-compatible types:</p> <pre><code>from marvin.ai_functions.entities import extract_named_entities\ntext = (\n'The United States passed a law that requires all cars to have a \"black'\n' box\" that records data about the car and its driver. The law is sponsored'\n\" by John Smith. It goes into effect in 2025.\"\n)\nextract_named_entities(text)\n# [\n#     NamedEntity(entity=\"United States\", type=\"GPE\"),\n#     NamedEntity(entity=\"John Smith\", type=\"PERSON\"),\n#     NamedEntity(entity=\"2025\", type=\"DATE\"),\n# ]\n</code></pre>"},{"location":"guide/ai_functions/entities/#extract-any-type-of-entity","title":"Extract any type of entity","text":"<p>A more flexible extraction function can retrieve multiple entity types in a single pass over the text. Here we pull countries and monetary values out of a sentence:</p> <pre><code>from pydantic import BaseModel\nclass Country(BaseModel):\nname: str\nclass Money(BaseModel):\namount: float\ncurrency: str\ntext = \"The United States EV tax credit is $7,500 for cars worth up to $50k.\"\nextract_types(text, types=[Country, Money])\n# [\n#     Country(name=\"United States\"),\n#     Money(amount=7500, currency=\"USD\"),\n#     Money(amount=50000, currency=\"USD\"),\n# ]\n</code></pre>"},{"location":"guide/ai_functions/strings/","title":"AI Functions for strings","text":""},{"location":"guide/ai_functions/strings/#fix-capitalization","title":"Fix capitalization","text":"<p>Given a string that may not have correct capitalization, fix its capitalization but make no other changes.</p> <pre><code>from marvin.ai_functions.strings import fix_capitalization\nfix_capitalization(\"the european went over to canada, eh?\")\n# The European went over to Canada, eh?\n</code></pre>"},{"location":"guide/ai_functions/strings/#apa-title-case","title":"APA title case","text":"<p>Return a title case string that you would want to use in a title.</p> <p>The Python string method <code>.title()</code> makes the first letter of every word uppercase and the remaing letters lowercase. This result isn't what you want to use for the title of a piece of writing, generally. <code>title_case</code> takes a string and returns a string you can use in a title.</p> <pre><code>from marvin.ai_functions.strings import title_case\ntitle_case(\"the european went over to canada, eh?\")\n# The European Went Over to Canada, Eh?\n</code></pre>"},{"location":"guide/ai_functions/strings/#rrules","title":"RRules","text":"<p>Generate RRules (structured objects that represent possibly recurring calendar events) from natural language.</p> <pre><code>from marvin.ai_functions.strings import rrule\nrrule(\"9am on the first business day of the quarter\")\n# \"RRULE:FREQ=MONTHLY;BYSETPOS=1;BYDAY=MO,TU,WE,TH,FR;BYMONTH=1,4,7,10;BYHOUR=9;BYMINUTE=0;BYSECOND=0;BYWEEKNO=1,14,27,40\"\n</code></pre>"},{"location":"guide/concepts/ai_functions/","title":"\ud83e\ude84 AI Functions","text":"<p>Features</p> <p>\ud83c\udf89 Create AI functions with a single <code>@ai_fn</code> decorator</p> <p>\ud83e\uddf1 Use native data structures (or Pydantic models!) as inputs and outputs</p> <p>\ud83d\udd17 Chain or nest calls to create functional AI pipelines</p> <p>\ud83e\uddd9 Add features to code that would be difficult or impossible to write yourself</p> <p>AI functions are functions that are defined locally but use AI to generate their outputs. Like normal functions, AI functions take arguments and return structured outputs like <code>lists</code>, <code>dicts</code> or even Pydantic models. Unlike normal functions, they don't need any source code! </p> <p>Consider the following example, which contains a function that generates a list of fruits. The function is defined with a descriptive name, annotated input and return types, and a docstring -- but doesn't appear to actually do anything. Nonetheless, because of the <code>@ai_fn</code> decorator, it can be called like a normal function and returns a list of fruits.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(n=3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre> <p>Tip</p> <p>AI functions work best with GPT-4, but results are still very good with GPT-3.5.</p> <p>... or another example, this time returning a finite set of <code>Literal</code> values to classify GitHub issues:</p> <pre><code>from typing_extensions import Literal\nfrom marvin import ai_fn\nIssueTag = Literal['bug', 'docs', 'enhancement', 'feature']\n@ai_fn\ndef issue_classifier(issue_body: str) -&gt; list[IssueTag]:\n\"\"\" returns appropriate issue tags given an issue body \"\"\"\nissue_classifier(\"\"\"\n    yeah so i tried using the tui and it teleported me to another dimension.\n    also there's a typo on the ai_fn's page, you forgot the code\n\"\"\")\n# ['bug', 'docs']\n</code></pre>"},{"location":"guide/concepts/ai_functions/#when-to-use-ai-functions","title":"When to use AI functions","text":"<p>Because AI functions look and feel just like normal functions, they are the easiest way to add AI capabilities to your code -- just write the definition of the function you want to call, and use it anywhere! However, though they can feel like magic, it's important to understand that there are times you should prefer not to use AI functions.</p> <p>Modern LLMs are extremely powerful, especially when working with natural language and ideas that are easy to discuss but difficult to describe algorithmically. However, since they don't actually execute code, computing extremely precise results can be surprisingly difficult. Asking an AI to compute an arithmetic expression is a lot like asking a human to do the same -- it's possible they'll get the right answer, but you'll probably want to double check on a calculator. On the other hand, you wouldn't ask the calculator to rewrite a paragraph as a poem, which is a perfectly natural thing to ask an AI. Bear in mind that AI functions are (relatively) slow and expensive compared to running code on your computer. </p> <p>Therefore, while there are many appropriate times to use AI functions, it's important to note that they complement normal functions incredibly well and to know when to use one or the other. AIs tend to excel at exactly the things that are very hard to describe algorithmically. If you're doing matrix multiplication, use a normal function. If you're extracting all the animals that are native to Europe from text, use an AI function.</p> <p>Here is a guide for when to use AI functions:</p> <ul> <li>Generating data (any kind of text, but also data matching a certain structure or template)</li> <li>Translating or rewriting text</li> <li>Summarization</li> <li>Sentiment analysis</li> <li>Keyword or entity extraction</li> <li>Asking qualitative questions about quantitative data</li> <li>Fixing spelling or other errors</li> <li>Generating outlines or action items</li> <li>Transforming one data structure to another</li> </ul> <p>Here is a guide for when NOT to use AI functions:</p> <ul> <li>The function is easy to write normally</li> <li>You want to be able to debug the function</li> <li>You require deterministic outputs</li> <li>Precise math beyond basic arithmetic</li> <li>You need any type of side effect or IO (AI functions are not \"executed\" in a traditional sense, so they can't interact with your computer or network)</li> <li>The objective is TOO magic (tempting though it may be, you can't write an AI function to solve an impossible problem)</li> </ul>"},{"location":"guide/concepts/ai_functions/#basic-usage","title":"Basic usage","text":"<p>The <code>ai_fn</code> decorator can be applied to any function. For best results, the function should have an informative name, annotated input types, a return type, and a docstring. The function does not need to have any source code written, but advanced users can add source code to influence the output in two different ways (see \"writing source code\")</p> <p>When a <code>ai_fn</code>-decorated function is called, all available information is sent to the AI, which generates a predicted output. This output is parsed and returned as the function result.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef my_function(input: Type) -&gt; ReturnType:\n\"\"\" \n    A docstring that describes the function's purpose and behavior.\n    \"\"\"\n# call the function\nmy_function(input=\"my input\")\n</code></pre> <p>Note the following:</p> <ol> <li>Apply the decorator to the function. It does not need to be called (though it can take optional arguments)</li> <li>The function should have a descriptive name</li> <li>The function's inputs should have type annotations</li> <li>The function's return type should be annotated</li> <li>The function has a descriptive docstring</li> <li>The function does not need any source code!</li> </ol>"},{"location":"guide/concepts/ai_functions/#advanced-usage","title":"Advanced usage","text":""},{"location":"guide/concepts/ai_functions/#customizing-the-llm","title":"Customizing the LLM","text":"<p>By default, AI functions use Marvin's global LLM settings. However, you can change this on a per-function basis by providing a valid model name or temperature to the <code>@ai_fn</code> decorator.</p> <p>For example, this function will always use GPT-3.5 with a temperature of 0.2. <pre><code>from marvin import ai_fn\n@ai_fn(llm_model='gpt-3.5-turbo', llm_temperature=0.2)\ndef my_function():\n...\n</code></pre></p>"},{"location":"guide/concepts/ai_functions/#deterministic-ai-functions","title":"Deterministic AI functions","text":"<p>LLM implementations like ChatGPT are non-deterministic; that is, they do not always return the same output for a given input. In some use cases, like natural conversation, this is desireable. In others, especially programmatic ones, it is not. You can control this by setting the model <code>temperature</code>. High temperature leads to greater variation in responses; a temperature of 0 will always give the same response for a given input. Marvin's default temperature is 0.8. To create a deterministic AI function, set its temperature to 0.</p> <p>Upgrading Marvin may change AI function outputs</p> <p>Marvin wraps your AI function with additional prompts in order to get the LLM to generate parseable outputs. We are constantly adjusting those prompts to improve performance and address edge cases. Therefore, even with a temperature of 0, what your AI function sends to the LLM might change if you upgrade from one version of Marvin to another, resulting in different AI function outputs. Therefore, AI functions with <code>temperature=0</code> are only guaranteed to be deterministic for a specific Marvin version. </p>"},{"location":"guide/concepts/ai_functions/#getting-values-at-runtime","title":"Getting values at runtime","text":"<p>By default, the <code>ai_fn</code> decorator will not attempt to run your function, even if it has source code. The LLM will attempt to predict everything itself. However, sometimes it is useful to manipulate the function at runtime. There are two ways to do this.</p> <p>The first is to wrap your function in another function or decorator. For example, here we write a private AI function to extract keywords from text and call it from a user-facing function that accepts a URL and loads the actual content.</p> <pre><code>import httpx\nfrom marvin import ai_fn\n@ai_fn\ndef _get_keywords(text:str) -&gt; list[str]:\n\"\"\"extract keywords from string\"\"\"\ndef get_keywords_from_url(url: str) -&gt; list[str]:\nresponse = httpx.get(url)\nreturn _get_keywords(response.content)\n</code></pre> <p>An advanced alternative is to yield from the ai function itself. This is supported for a single yield statement:</p> <pre><code>import httpx\nfrom marvin import ai_fn\n@ai_fn\ndef get_keywords_from_url(url: str) -&gt; list[str]:\n\"\"\"Extract keywords from the text of a URL\"\"\"\nresponse = httpx.get(url)\nyield response.content\n</code></pre> <p>In this case, the function will be run up to the yield statement, and the yielded value will be considered by the LLM when generating a result. Therefore, these two examples are equivalent. It may be that the first example is more legible than the second, which is extremely magical.</p>"},{"location":"guide/concepts/ai_functions/#async-functions","title":"Async functions","text":"<p>The <code>ai_fn</code> decorator works with async functions.</p> <pre><code>from marvin import ai_fn\n@ai_fn\nasync def f(x: int) -&gt; int:\n\"\"\"Add 100 to x\"\"\"\nawait f(5)\n</code></pre>"},{"location":"guide/concepts/ai_functions/#complex-annotations","title":"Complex annotations","text":"<p>Annotations don't have to be types; they can be complex objects or even string descriptions. For inputs, the annotation is transmitted to the AI as-is. Return annotations are processed through Marvin's <code>ResponseFormatter</code> mechanism, which puts extra emphasis on compliance. This means you can supply complex instructions in your return annotation. However, note that you must include the word <code>json</code> in order for Marvin to automatically parse the result into native objects!</p> <p>Therefore, consider these two approaches to defining an output: <pre><code>from marvin import ai_fn\n@ai_fn\ndef fn_with_docstring(n: int) -&gt; list[dict]:\n\"\"\"\n    Generate a list of n people with names and ages\n    \"\"\"\n@ai_fn\ndef fn_with_string_annotation(n: int) -&gt; 'a json list of dicts that have keys for name and age':\n\"\"\"\n    Generate a list of n people\n    \"\"\"\nclass Person(pydantic.BaseModel):\nname: str\nage: int\n@ai_fn\ndef fn_with_structured_annotation(n: int) -&gt; list[Person]:\n\"\"\"\n    Generate a list of n people\n    \"\"\"\n</code></pre> All three of these functions will give similar output (though the last one, <code>fn_with_structured_annotation</code>, will return Pydantic models instead of dicts). However, they are increasingly specific in their instructions to the AI. While you should always try to make your intent as clear as possible to the AI, you should also choose an approach that will make sense to other people reading your code. This would lead us to probably prefer the first or third functions over the second, which doesn't look like a typical Python function.</p>"},{"location":"guide/concepts/ai_functions/#plugins","title":"Plugins","text":"<p>AI functions are powered by bots, so they can also use plugins. Unlike bots, AI functions have no plugins available by default in order to minimize the possibility of confusing behavior. You can provide plugins when you create the AI function:</p> <pre><code>@ai_fn(plugins=[...])\ndef my_function():\n...\n</code></pre>"},{"location":"guide/concepts/ai_functions/#examples","title":"Examples","text":""},{"location":"guide/concepts/ai_functions/#generate-a-list-of-fruits","title":"Generate a list of fruits","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#generate-fake-data-according-to-a-schema","title":"Generate fake data according to a schema","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fake_people(n: int) -&gt; list[dict]:\n\"\"\"\n    Generates n examples of fake data representing people, \n    each with a name and an age.\n    \"\"\"\nfake_people(3)\n# [{'name': 'John Doe', 'age': 29},\n#  {'name': 'Jane Smith', 'age': 34},\n#  {'name': 'Alice Johnson', 'age': 42}]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#correct-spelling-and-grammar","title":"Correct spelling and grammar","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fix_sentence(sentence: str) -&gt; str:\n\"\"\"\n    Fix all grammatical and spelling errors in a sentence\n    \"\"\"\nfix_sentence(\"he go to mcdonald and buy burg\") # \"He goes to McDonald's and buys a burger.\"\n</code></pre>"},{"location":"guide/concepts/ai_functions/#cleaning-data","title":"Cleaning data","text":"<p>Cleaning data is such an important use case that Marvin has an entire module dedicated to it, including AI functions for categorization, standardization, entity extraction, and context-aware fills for missing values. See the data cleaning documentation for more information.</p>"},{"location":"guide/concepts/ai_functions/#unit-testing-llms","title":"Unit testing LLMs","text":"<p>One of the difficulties of building an AI library is unit testing it! While it's possible to make LLM outputs deterministic by setting the temperature to zero, a small change to a prompt could result in very different outputs. Therefore, we want a way to assert that an LLM's output is \"approximately equal\" to an expected value.</p> <p>This example is actually used by Marvin itself! See <code>marvin.utilities.tests.assert_llm()</code>.</p> <pre><code>@ai_fn()\ndef assert_llm(output: Any, expectation: Any) -&gt; bool:\n\"\"\"\n    Given the `output` of an LLM and an expectation, determines whether the\n    output satisfies the expectation.\n    For example:\n        `assert_llm(5, \"output == 5\")` will return `True` \n        `assert_llm(5, 4)` will return `False` \n        `assert_llm([\"red\", \"orange\"], \"a list of colors\")` will return `True` \n        `assert_llm([\"red\", \"house\"], \"a list of colors\")` will return `False`\n    \"\"\"\nassert_llm('Hello, how are you?', expectation='Hi there') # True\n</code></pre>"},{"location":"guide/concepts/ai_functions/#summarize-text","title":"Summarize text","text":"<p>This function takes any text and summarizes it. See the next example for a function that can also access Wikipedia automatically.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize(text: str) -&gt; str:\n\"\"\"\n    Summarize the provided text\n    \"\"\"\nimport wikipedia\npage = wikipedia.page('large language model')\nsummarize(text=page.content)\n# Large language models (LLMs) are neural networks with billions of parameters\n# trained on massive amounts of unlabelled text. They excel at various tasks and\n# can capture much of human language's syntax and semantics. LLMs use the\n# transformer architecture and are trained using unsupervised learning. Their\n# applications include fine-tuning and prompting for specific natural language\n# processing tasks.\n</code></pre>"},{"location":"guide/concepts/ai_functions/#summarize-text-after-loading-a-wikipedia-page","title":"Summarize text after loading a Wikipedia page","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function downloads content from Wikipedia given a title.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize_from_wikipedia(title: str) -&gt; str:\n\"\"\"\n    Loads the wikipedia page corresponding to the provided \n    title and returns a summary of the content.\n    \"\"\"\nimport wikipedia\npage = wikipedia.page(title)\n# the content to summarize\nyield page.content\nsummarize_from_wikipedia(title='large language model')\n# A large language model (LLM) is a language model consisting of a neural\n# network with many parameters (typically billions of weights or more), trained\n# on large quantities of unlabelled text using self-supervised learning. LLMs\n# emerged around 2018 and perform well at a wide variety of tasks. This has\n# shifted the focus of natural language processing research away from the\n# previous paradigm of training specialized supervised models for specific\n# tasks.\n</code></pre>"},{"location":"guide/concepts/ai_functions/#suggest-a-title-after-loading-a-url","title":"Suggest a title after loading a URL","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function loads an article and then suggests a title for it.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef suggest_title(url: str) -&gt; str:\n\"\"\"\n    Suggests a title for the article found at the provided URL\n    \"\"\"\nimport httpx\n# load the url\nresponse = httpx.get(url)\n# return the url contents \nyield marvin.utilities.strings.html_to_content(response.content)\nsuggest_title(url=\"https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/\")\n# OpenAI Releases GPT-4: State-of-the-Art AI Model with Improved Image and Text Understanding\n</code></pre>"},{"location":"guide/concepts/ai_functions/#generate-rhymes","title":"Generate rhymes","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef rhyme(word: str) -&gt; str:\n\"\"\"\n    Generate a word that rhymes with the supplied `word`\n    \"\"\"\nrhyme(\"blue\") # glue\n</code></pre>"},{"location":"guide/concepts/ai_functions/#find-words-meeting-specific-criteria","title":"Find words meeting specific criteria","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef find_words(text: str, criteria: str) -&gt; list[str]:\n\"\"\"\n    Given text and some criteria, returns a list of \n    every word meeting that criteria.\n    \"\"\"\ntext = \"The quick brown fox jumps over the lazy dog.\"\nfind_words(text, criteria=\"adjectives\") # [\"quick\", \"brown\", \"lazy\"]\nfind_words(text, criteria=\"colors\") # [\"brown\"]\nfind_words(text, criteria=\"animals that aren't dogs\") # [\"fox\"]\n</code></pre>"},{"location":"guide/concepts/ai_functions/#suggest-emojis","title":"Suggest emojis","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef get_emoji(text: str) -&gt; str:\n\"\"\"\n    Returns an emoji that describes the provided text.\n    \"\"\"\nget_emoji(\"incredible snack\") # '\ud83c\udf7f'\n</code></pre>"},{"location":"guide/concepts/ai_functions/#generate-rrules","title":"Generate RRULEs","text":"<p>RRULE strings are standardized representations of calendar events. This AI function can convert natural language into an RRULE.</p> <p>This is also available as a builtin function: <code>marvin.ai_functions.strings.rrule</code></p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef rrule(text: str) -&gt; str:\n\"\"\"\n    Generate valid RRULE strings from a natural language description of an event\n    \"\"\"\nyield pendulum.now.isoformat()\nrrule('every hour from 9-6 on thursdays')\n# \"RRULE:FREQ=WEEKLY;BYDAY=TH;BYHOUR=9,10,11,12,13,14,15,16;BYMINUTE=0;BYSECOND=0\"\n</code></pre>"},{"location":"guide/concepts/ai_functions/#get-a-datetime-from-a-natural-language-description","title":"Get a datetime from a natural language description","text":"<pre><code>from datetime import datetime\nfrom marvin import ai_fn\n@ai_fn\ndef make_datetime(description: str, tz: str = \"BST\") -&gt; datetime:\n\"\"\" generates a datetime from a description \"\"\"\n# !date +\"%Y-%m-%d %T %Z\"\n# 2023-06-23 22:30:25 BST\ndt = make_datetime(\"5 mins from now\")\n# datetime.datetime(2023, 6, 23, 22, 35, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600)))\ndt.isoformat()\n# '2023-06-23T22:35:00+01:00'\n</code></pre>"},{"location":"guide/concepts/ai_models/","title":"\ud83e\ude84 AI Models","text":"<p>Features</p> <p>\ud83c\udf89 Create AI models with a single <code>@ai_model</code> decorator</p> <p>\ud83e\uddf1 Define Pydantic models that work with both structured data and unstructured text</p> <p>\ud83d\udd17 Use AI models to transform raw text into type-safe outputs</p> <p>\ud83e\uddd9 Enhance your data schema with AI capabilities that would be difficult or impossible to implement manually</p> <p>AI models are Pydantic models that are defined locally and use AI to process their inputs at runtime. Like normal Pydantic models, AI models define a schema that data must comply with. Unlike normal Pydantic models, AI models can handle unstructured text and automatically convert it into structured, type-safe outputs without requiring any additional source code!</p> <p>With Marvin, you use Pydantic to shape your data model as usual and enhance your model with <code>@ai_model</code>. This decorator imparts an extraordinary capability to your Pydantic model: the capability to manage unstructured text.</p> <pre><code>from marvin import ai_model\nimport pydantic\nfrom typing import Optional\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\nResume('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io').json(indent = 2)\n#{\n#     first_name: 'Ford',\n#     last_name: 'Prefect',\n#     email: 'ford@prefect.io',\n#     phone: '(555) 5124-5242',\n#}\n</code></pre> <p>Tip</p> <p>AI models work best with GPT-4, but results are still very good with GPT-3.5.</p>"},{"location":"guide/concepts/ai_models/#when-to-use-ai-models","title":"When to use AI Models","text":"<p>Because AI models integrate seamlessly with the Pydantic framework, they are a straightforward way to infuse AI capabilities into your data processing pipeline. Just define the Pydantic model with the fields you want to extract from the unstructured text and use it anywhere! However, even though they can feel like magic, it's crucial to understand that there are situations where you might prefer not to use AI models.</p> <p>Modern LLMs are extraordinarily potent, particularly when dealing with natural language and concepts that are simple to express but challenging to encode algorithmically. However, because they don't actually execute code, computing precise results can be tricky. </p> <p>For example, asking an AI to summarize intricate legal language is akin to asking a human to do the same -- it's feasible they'll comprehend the right context, but you'll probably want to double-check with a legal expert. On the other hand, you wouldn't ask the legal expert to summarize a complex research paper, which is a perfectly natural thing to ask an AI. </p> <p>Therefore, while there are many suitable times to use AI models, they aren't always a great fit. AI models tend to excel at exactly the things that are hard to codify algorithmically. If you're performing simple data validation, use a normal Pydantic model. If you're extracting context from unstructured text, use an AI model.</p>"},{"location":"guide/concepts/ai_models/#basic-usage","title":"Basic usage","text":"<p>The <code>ai_model</code> decorator can be applied to any Pydantic model. For optimal results, the model should have a descriptive name, annotated fields, and a class docstring. The model does not need to have any pre-processing or post-processing methods written. However, advanced users can add these methods to influence the output.</p> <p>When an <code>ai_model</code>-decorated class is instantiated with unstructured text, all available information is sent to the AI. The AI generates a predicted output that is parsed according to the model's schema and returned as an instance of the model.</p> <pre><code>from marvin import ai_model\nimport pydantic\nfrom typing import Optional\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: Optional[str]\nemail: str\nResume('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io').json(indent = 2)\n#{\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#advanced-usage","title":"Advanced usage","text":"<p>Under the hood, AI Models use AI Functions to extract data before it's passed to Pydantic's validation rules. Unsurprisingly then, AI Models expose the same advanced customizations that AI Functions do. </p> <pre><code>@ai_model(llm_model='gpt-3.5-turbo', llm_temperature=0.2)\nclass MyFirstModel(pydantic.BaseModel):\n...\n</code></pre> <p>You can customize the LLM's temperature, or give your AI Model access to Wikipedia,  internal documentation, or a sanitized executable environment by using Marvin's plugins. AI models have no plugins available by default in order to minimize the possibility of confusing behavior. See AI function docs. </p>"},{"location":"guide/concepts/ai_models/#examples","title":"Examples","text":""},{"location":"guide/concepts/ai_models/#structure-conversational-user-input","title":"Structure conversational user input","text":"<pre><code>from marvin import ai_model\nfrom typing import Optional, List\nimport datetime\nimport pydantic\nclass Destination(pydantic.BaseModel):\nstart: datetime.date\nend: datetime.date\ncity: Optional[str]\ncountry: str\nsuggested_attractions: list[str]\n@ai_model\nclass Trip(pydantic.BaseModel):\ntrip_start: datetime.date\ntrip_end: datetime.date\ntrip_preferences: list[str]\ndestinations: List[Destination]\nTrip('''\\\nI've got all of June off, so hoping to spend the first\\\nhalf of June in London and the second half in Rabat. I love \\\ngood food and going to museums.\n''').json(indent = 2)\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#format-electronic-health-records-data-declaratively","title":"Format electronic health records data declaratively","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nclass Patient(BaseModel):\nname: str\nage: int\nis_smoker: bool\nclass Diagnosis(BaseModel):\ncondition: str\ndiagnosis_date: date\nstage: Optional[str] = None\ntype: Optional[str] = None\nhistology: Optional[str] = None\ncomplications: Optional[str] = None\nclass Treatment(BaseModel):\nname: str\nstart_date: date\nend_date: Optional[date] = None\nclass Medication(Treatment):\ndose: Optional[str] = None\nclass BloodTest(BaseModel):\nname: str\nresult: str\ntest_date: date\n@ai_model\nclass PatientData(BaseModel):\npatient: Patient\ndiagnoses: List[Diagnosis]\ntreatments: List[Treatment]\nblood_tests: List[BloodTest]\nPatientData('''\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n''').json(indent = 2)\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#text-to-orm","title":"Text to ORM","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\nclass DjangoLookup(BaseModel):\nfield: Literal[*django_fields]\nlookup: Literal[*django_lookups] = pydantic.Field(description = 'e.g. __iregex')\nvalue: Any\n@ai_model\nclass DjangoQuery(BaseModel):\n''' A model represneting a Django ORM query'''\nlookups: List[DjangoLookup]\ndef to_q(self) -&gt; Q:\nq = Q()\nfor lookup in self.lookups:\nq &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\nreturn q\nDjangoQuery('''\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days'''\n).to_q()\n# &lt;Q: (AND: \n#     ('date_joined__lte', '2023-03-11'), \n#     ('last_purchase_date__isnull', False), \n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"guide/concepts/ai_models/#extract-financial-information-from-messy-csv-data","title":"Extract financial information from messy CSV data","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n@ai_model\nclass CapTable(pydantic.BaseModel):\ntotal_authorized_shares: int\ntotal_common_share: int\ntotal_common_shares_outstanding: Optional[int]\ntotal_preferred_shares: int\nconversion_price_multiple: int = 1\nCapTable('''\\\nIn the cap table for Charter, the total authorized shares amount to 13,250,000. \nThe total number of common shares stands at 10,000,000 as specified in Article Fourth, \nclause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \navailable at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \nin Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \nset at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \nderived from the Term Sheet.\\\n''').json(indent = 2)\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#extract-action-items-from-meeting-transcripts","title":"Extract action items from meeting transcripts","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\nclass ActionItem(pydantic.BaseModel):\nresponsible: str\ndescription: str\ndeadline: Optional[datetime.datetime]\ntime_sensitivity: Literal['low', 'medium', 'high']\n@ai_model\nclass Conversation(pydantic.BaseModel):\n'''A class representing a team converastion'''\nparticipants: List[str]\naction_items: List[ActionItem]\nConversation('''\nAdam: Hey Jeremiah can you approve my PR? I requested you to review it.\nJeremiah: Yeah sure, when do you need it done by?\nAdam: By this Friday at the latest, we need to ship it by end of week.\nJeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\nNate: Jeremiah we can meet today to chat.\nJeremiah: Okay, I'll book something for today.\n''').json(indent = 2)\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#schema-normalization-for-data-warehousing","title":"Schema normalization for data warehousing","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\nclass YourSchema(pydantic.BaseModel):\n'''A profile representing a user'''\nfirst_name: str\nlast_name: str\nphone_number: str\nemail: str\ndate_joined: datetime.datetime\n@ai_model\nclass MySchema(pydantic.BaseModel):\n'''A profile representing a user'''\ngiven_name: str\nfamily_name: str\ncontact_number: str\nemail_address: str\ndatetime_created: datetime.datetime\n# I want the data in my schema.\nMySchema(\n# But I only have data from your schema.\nYourSchema(\nfirst_name = 'Ford',\nlast_name = 'Prefect',\nphone_number = '555-555-5555',\nemail = 'ford@prefect.io',\ndate_joined ='2022-05-11T23:59:59'\n).json()\n).json(indent = 2)\n# {\n#   \"given_name\": \"Ford\",\n#   \"family_name\": \"Prefect\",\n#   \"contact_number\": \"555-555-5555\",\n#   \"email_address\": \"ford@prefect.io\",\n#   \"datetime_created\": \"2022-05-11T23:59:59\"\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#structure-data-from-scraping-web-pages","title":"Structure data from scraping web pages","text":"<pre><code>from marvin import ai_model\nimport pydantic\nimport requests\nfrom bs4 import BeautifulSoup as soup\n@ai_model\nclass Company(pydantic.BaseModel):\nname: str\nindustries: List[str]\ndescription_short: str\ndescription_long: str\nproducts: List[str]\nresponse = requests.get('https://www.apple.com')\ntext = soup(response.content).get_text(separator = ' ', strip = True)\nCompany(text).json(indent = 2)\n# {\n#   \"name\": \"Apple\",\n#   \"industries\": [\n#     \"Technology\",\n#     \"Consumer electronics\"\n#   ],\n#   \"description_short\": \"Apple is a multinational technology company that designs, develops, and sells consumer electronics, computer software, and online services.\",\n#   \"description_long\": \"Apple Inc. is an American multinational technology company that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker. Apple's software includes the macOS, iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites. The online services include the iTunes Store, the iOS App Store, and Mac App Store, Apple Music, and iCloud. The company was founded on April 1, 1976, and incorporated on January 3, 1977, by Steve Jobs, Steve Wozniak, and Ronald Wayne.\",\n#   \"products\": [\n#     \"iPhone\",\n#     \"iPad\",\n#     \"Mac\",\n#     \"iPod\",\n#     \"Apple Watch\",\n#     \"Apple TV\",\n#     \"HomePod\",\n#     \"macOS\",\n#     \"iOS\",\n#     \"iPadOS\",\n#     \"watchOS\",\n#     \"tvOS\",\n#     \"iTunes\",\n#     \"Safari\",\n#     \"iLife\",\n#     \"iWork\",\n#     \"iTunes Store\",\n#     \"iOS App Store\",\n#     \"Mac App Store\",\n#     \"Apple Music\",\n#     \"iCloud\"\n#   ]\n# }\n</code></pre>"},{"location":"guide/concepts/ai_models/#smart-routing-in-application-development","title":"Smart routing in application development","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\n@ai_model\nclass Router(pydantic.BaseModel):\n'''A class representing an AI-based router'''\nrequest: str = pydantic.Field(description = 'Raw user query')\npage: Literal['/profile', '/billing', '/metrics', '/refunds']\nRouter('''I want to update my address''')\n# {\n#   \"request\": \"I want to update my address\",\n#   \"page\": \"/profile\"\n# }\n</code></pre>"},{"location":"guide/concepts/bots/","title":"\ud83e\udd16 Bots","text":"<p>Features</p> <p>\ud83e\udd16 Create bots with distinct personalities and instructions</p> <p>\ud83d\udd0c Use plugins to give bots new capabilities</p> <p>\ud83d\udcac Persistent memories let you resume threads from different sessions</p> <p>\ud83d\udce1 Talk to bots from Python, your CLI, or the Marvin REST API</p> <p>One of Marvin's central abstractions is the <code>Bot</code> class. A bot is an interface to send text to an AI and receive a response that aligns with the user's objective as much as possible. Marvin allows users to customize this behavior in ways that can transform bots from \"AI assistants\" to reusable programs.</p>"},{"location":"guide/concepts/bots/#when-to-use-bots","title":"When to use bots","text":"<p>Bots are different than AI functions and, in some ways, are more powerful. In fact, AI functions are actually powered by bots. AI functions are designed to take well-scoped problems and turn them into familiar, reusable functions. While bots can be used for the same purpose, they are more appropriate for complex, multi-step interactions or problem solving. </p> <p>AI functions are designed to make the AI invisible. Bots bring the AI to the forefront.</p>"},{"location":"guide/concepts/bots/#python","title":"Python","text":""},{"location":"guide/concepts/bots/#interactive-use","title":"Interactive use","text":"<p>To create a bot, instantiate the bot class.  <pre><code>from marvin import Bot\nford_bot = Bot(\nname=\"Ford\", \npersonality=\"Can't get the hang of Thursdays\", \ninstructions=\"Always responds as if researching an article for the Hitchhiker's Guide to the Galaxy\"\n)\n</code></pre></p> <p>You can immediately talk to the bot by calling its <code>say()</code> method, which is an async coroutine.</p> <pre><code>await ford_bot.say(\"Hello!\")\n</code></pre>"},{"location":"guide/concepts/bots/#history","title":"History","text":"<p>When you speak with a bot, every message is automatically stored. The bot uses its <code>history</code> module to access these messages, which means you can refer to earlier parts of your conversation without any extra work. </p> <p>In Marvin, each conversation is called a <code>thread</code>. Bots generate a new thread each time they are instantiated.  </p> <p>Clear the thread and start a new one by calling <code>Bot.reset_thread()</code>. </p> <p>Resume a specific thread by calling <code>Bot.set_thread()</code>.</p>"},{"location":"guide/concepts/bots/#saving-bots","title":"Saving bots","text":"<p>Bots can be saved to the database by calling the <code>Bot.save()</code> method. Bots are saved by name. </p> <pre><code>bot = Bot(\"Ford\")\nawait bot.save()\n</code></pre> <p>Use the <code>if_exists</code> argument to customize what happens if a bot with the same name already exists:</p> <ul> <li><code>None</code> (the default): an error will be raised.</li> <li><code>\"delete\"</code>: the old bot will be deleted and a new bot will be saved. The new bot will not share anything with the old bot, including its message history.</li> <li><code>\"update\"</code>: the old bot will be updated with the new bot's details. The ID and message history will be preserved.</li> <li><code>\"cancel\"</code>: no changes are made and the old bot is left untouched, but no error is raised.</li> </ul> <pre><code>bot = Bot(\"Ford\")\nawait bot.save(if_exists=\"update\")\n</code></pre>"},{"location":"guide/concepts/bots/#loading-bots","title":"Loading bots","text":"<p>Bots can be loaded with the <code>Bot.load()</code> method.</p> <pre><code>bot = await Bot.load(\"Ford\")\n</code></pre>"},{"location":"guide/concepts/bots/#interactive-chats","title":"Interactive Chats","text":"<p>To quickly jump into an interactive chat with a bot:</p> <pre><code>bot.interactive_chat()\n</code></pre> <p>This will launch the Marvin TUI with the current bot already selected. Note this will also save the bot to the database, overwriting any existing bot with the same name. </p> <p>To launch a simpler experience, pass <code>tui=False</code> when you call the function.</p>"},{"location":"guide/concepts/bots/#streaming-responses","title":"Streaming responses","text":"<p>By default, bots process an entire response and return it as a structured object. However, you can get a streaming response by providing a custom <code>on_token_callback</code> function. Every time the bot generates a token, the callback will be called with a buffer of all the tokens generated to that point. You can access the most recent token as <code>buffer[-1]</code>. </p> <p>For example, to print each token as it's generated: <pre><code>bot = marvin.Bot()\nawait bot.say(\"Hello!\", on_token_callback=lambda buffer: print(buffer[-1]))\n</code></pre></p>"},{"location":"guide/concepts/bots/#async-and-sync-methods","title":"Async and sync methods","text":"<p>Above you've seen how to use asynchronous methods. If needed, synchronous convenience methods are also provided. Just append <code>_sync</code> to the asynchronous method name.</p> <p>Synchronous substitue for async <code>say()</code> method: <pre><code>ford_bot.say_sync(\"Hello again!\")\n</code></pre></p> <p>Synchronous substitute for async <code>save()</code> method: <pre><code>bot = Bot(\"Ford\")\nbot.save_sync()\n</code></pre></p> <p>Synchronous substitute for async <code>load()</code> method: <pre><code>bot = Bot.load_sync(\"Ford\")\n</code></pre></p> <p>Callbacks can be synchronous or asynchronous, and both can be used with the bot's async <code>say()</code> or synchronous <code>say_sync()</code> methods.</p>"},{"location":"guide/concepts/bots/#tui","title":"TUI","text":""},{"location":"guide/concepts/bots/#interactive-use_1","title":"Interactive use","text":"<p>To chat with a bot from your terminal, run <code>marvin chat</code></p> <p></p>"},{"location":"guide/concepts/bots/#loading-an-existing-bot","title":"Loading an existing bot","text":"<p>If you have saved a bot, you can load it in the CLI by using the <code>-b</code> flag and providing the bot's name:</p> <pre><code>marvin chat -b Arthur\n</code></pre>"},{"location":"guide/concepts/bots/#customization","title":"Customization","text":""},{"location":"guide/concepts/bots/#name","title":"Name","text":"<p>Names are unique identifiers that make it easy to reference a specific bot.</p>"},{"location":"guide/concepts/bots/#instructions","title":"Instructions","text":"<p>Instructions define the bot's behavior by specifying how it should respond to questions. For example, the default instructions are to assist the user. However, more utilitarian bots might be instructed to only respond with JSON (or with a specific JSON schema), extract keywords, always rhyme, etc. Bots, especially GPT-4 bots, should not go against their instructions at any time.</p> <p>!!! \"Customizing Instruction Templates\"     While the default instruction template serves most Bot use-cases well, it can be customized by passing a <code>instructions_template: str</code> to a <code>Bot</code>. Note that     the following variables will still be passed to your instructions template when the bot retrieves its instructions: <code>name</code>, <code>instructions</code>, <code>response_format</code>,      <code>personality</code>, <code>date</code>. Warning: Custom instruction templates may cause incompatibilities with future Marvin versions.</p>"},{"location":"guide/concepts/bots/#personality","title":"Personality","text":"<p>Personality affects the style of a bot's responses. For example different bots can have different tones, senses of humor, or frequency of confirmation checks. Personality can include a persona or role as well as interests, fears, or broad objectives. </p> <p>By combining personality and instructions, bot instances can produce complex behavior that can be very different from what users might expect from a chat interface. For example, you could instruct a bot to always respond in a certain way, but use personality to have it act a role (such as a coach, engineer, or therapist).</p>"},{"location":"guide/concepts/bots/#plugins","title":"Plugins","text":"<p>Plugins allow bots to access new information and functionality. By default, bots have plugins that let them browse the internet, visit URLs, and run simple calculations.</p>"},{"location":"guide/concepts/bots/#formatting-responses","title":"Formatting responses","text":"<p>You can optionally enforce certain formats for the bot's responses. In some cases, you can also validate and even parse the resulting output into native objects.</p> <p>Please note: complex response formatting is significantly better with GPT-4 than GPT-3.5.</p> <p>To set up formatting, you need to supply a <code>ResponseFormatter</code> object that defines formatting, validation, and parsing. </p> <p>As a convenience, Marvin also supports a shorthand way of defining formats that will let the library select the most appropriate <code>ResponseFormatter</code> automatically. Shorthand formats can include natural language descriptions, Python types, JSON instructions, or Pydantic models. </p> <p>Here are examples of various shorthand formats:</p>"},{"location":"guide/concepts/bots/#python-types","title":"Python types","text":"<p>Supply Python types to have the bot enforce the appropriate return types. Python types are always validated and parsed.</p> <pre><code>bot = Bot(response_format=bool)\nresponse = await bot.say('Are these statements equivalent? 1: The coffee is hot. 2: The coffee is scalding.')\nprint(response.parsed_content) # True\n</code></pre> <pre><code>bot = Bot(response_format=list[dict[str, int]])\nresponse = await bot.say(\"Format this: (x is 1, y is two), (a is 3, b is 4)\")\nprint(response.parsed_content) # [{'x': 1, 'y': 2}, {'a': 3, 'b': 4}]\n</code></pre>"},{"location":"guide/concepts/bots/#natural-language","title":"Natural language","text":"<p>You can describe the format you want the bot to use, and it will do its best to follow your instructions. If your description includes the word \"json\", then it will also be parsed and validated (you can enable this explicitly by providing a <code>JSONResponseFormatter</code>).</p> <pre><code>bot = Bot(response_format=\"a JSON list of strings\")\nresponse = await bot.say(\"Which of these are capitalized: Apple, banana, cherry, Date, elephant\")\nprint(response.parsed_content) # [\"Apple\", \"Date\"]\n</code></pre> <pre><code>bot = Bot(response_format=\"a hyphenated list\")\nresponse = await bot.say(\"Which of these are capitalized: Apple, banana, cherry, Date, elephant\")\nprint(response.parsed_content) # '- Apple\\n- Date'\n</code></pre> <pre><code>bot = Bot(response_format='&lt;animal&gt; like to eat &lt;food&gt; and live in &lt;biome&gt;')\nresponse = await bot.say('tell me about foxes')\nprint(response.parsed_content) # \"Foxes like to eat small mammals and live in forests.\"\n</code></pre>"},{"location":"guide/concepts/bots/#pydantic","title":"Pydantic","text":"<pre><code>class MyFormat(BaseModel):\nx: int\ny: str = Field(\"The written form of x\")\nbot = Bot(response_format=MyFormat)\nresponse = await bot.say(\"Generate output where x is 22\")\nprint(response.parsed_content) # MyFormat(x=22, y='Twenty-two')\n</code></pre>"},{"location":"guide/concepts/infra/","title":"\u2699\ufe0f Infra","text":"<p>Construction zone</p> <p>This area of the docs is under active development and may change.</p>"},{"location":"guide/concepts/infra/#databases","title":"Databases","text":""},{"location":"guide/concepts/infra/#sqlite","title":"Sqlite","text":"<p>By default, Marvin uses a Sqlite database located at <code>~/.marvin/marvin.sqlite</code>. You can customize this by setting <code>MARVIN_DATABASE_CONNECTION_URL</code> to <code>sqlite+aiosqlite:////{path/to/database}</code>.</p>"},{"location":"guide/concepts/infra/#postgres","title":"Postgres","text":"<p>Marvin can also use Postgres (though this isn't as actively tested at this time). To do so, install the postgres extra: <code>pip install \"marvin[postgres]\"</code> and set <code>MARVIN_DATABASE_CONNECTION_URL</code> to <code>postgresql+asyncpg://{username}:{password}@{host}:{port}/{database}</code>, filling all variables appropriately.</p>"},{"location":"guide/concepts/infra/#postgres-on-gcp-cloud-sql","title":"Postgres on GCP Cloud SQL","text":"<p>If you want to connect to Postgres on GCP Cloud SQL</p>"},{"location":"guide/concepts/infra/#locally","title":"locally","text":"<p>You can connect to Cloud SQL by using cloud-sql-proxy and set the <code>MARVIN_DATABASE_CONNECTION_URL</code> in the <code>.env</code> file.</p> <pre><code>MARVIN_DATABASE_CONNECTION_URL=\"postgresql+asyncpg:/{username}:{password}@localhost:5432/{database}\"\n</code></pre>"},{"location":"guide/concepts/infra/#through-another-gcp-service","title":"through another GCP service","text":"<p><code>MARVIN_DATABASE_CONNECTION_URL</code> could be like</p> <p><pre><code>MARVIN_DATABASE_CONNECTION_URL=\"postgresql+asyncpg://{username}:{password}@/{database}?host=/cloudsql/{project}:{region}:{instance}\"\n</code></pre> For more detail, see the GCP SQL docs.</p>"},{"location":"guide/concepts/infra/#migrations","title":"Migrations","text":"<p>Marvin keeps the database schema up-to-date with Alembic migrations. If Marvin detects an empty database, it will run the initial migration update automatically. However, subsequent migrations will not be run automatically (to avoid any conflicts). Instead, Marvin checks to see if the database is up-to-date on startup and prints a warning if it isn't. You can disable this behavior by setting <code>MARVIN_DATABASE_CHECK_MIGRATION_VERSION_ON_STARTUP=0</code>.</p> <p>After upgrading Marvin, or when you see the warning described above, you should upgrade the database by running:</p> <pre><code>marvin database upgrade\n</code></pre> <p>You will be asked to confirm the upgrade; pass <code>-y</code> to do so automatically (this can be useful in CI). The upgrade command is idempotent and safe to run multiple times; the database is only modified if necessary.</p>"},{"location":"guide/concepts/infra/#chroma","title":"Chroma","text":"<p>Marvin provides a simple wrapper of the ChromaDB client to make it easier to interact with the database.</p> <p>ChromaDB has a large memory footprint and is an optional dependency</p> <p>ChromaDB uses <code>sentence-transformers</code> by default for embeddings, which requires <code>torch</code>. <code>torch</code> has recently added wheels for Python 3.11.</p> <p>Although Marvin uses OpenAI's \"text-embedding-ada-002\" model offered via <code>chromadb.utils.embedding_functions</code>, <code>chromadb</code> enforces the <code>sentence-transformers</code> dependency at this time.</p> <p>Install the <code>chromadb</code> extra with <code>pip install marvin[chromadb]</code> to use ChromaDB.</p> <p>Read the ChromaDB usage guide for more information.</p>"},{"location":"guide/concepts/infra/#relevance-to-marvin","title":"Relevance to Marvin","text":"<p>ChromaDB is an embeddings database that is used by Marvin to store and query document embeddings.</p> <p>When you call <code>.load_and_store()</code> on a <code>Loader</code>, you are calling <code>Chroma.add</code> to store documents in the default collection.</p> <p><code>load_and_store</code> accepts an optional <code>topic_name</code> that corresponds to a collection in ChromaDB. If you want to store documents in a different collection, simply pass a different <code>topic_name</code> to <code>load_and_store</code> and the collection will be created for you or updated.</p>"},{"location":"guide/concepts/infra/#usage","title":"Usage","text":"<p>If desired, you can use it directly:</p>"},{"location":"guide/concepts/infra/#querying","title":"Querying","text":"<pre><code>from marvin.infra.chroma import Chroma\nasync with Chroma() as chroma:\nquery_results: dict[str, list] = await chroma.query(\nquery_texts=[\"some natural language query\"],\nwhere={\"some_metadata_field\": \"has_this_value\"},\ninclude=[\"documents\", \"metadatas\"], # \"ids\" are always included\n)\n</code></pre>"},{"location":"guide/concepts/infra/#adding","title":"Adding","text":"<pre><code>from marvin.infra.chroma import Chroma\nasync with Chroma(collection_name=\"my-new-collection\") as chroma:\nawait chroma.add(\ndocuments=[\"some text\", \"some other text\"],\nmetadatas=[{\"some_metadata_field\": \"some_value\"}, {\"some_metadata_field\": \"some_other_value\"}],\n)\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/","title":"\ud83c\udfd7\ufe0f Loaders &amp; Documents","text":"<p>Construction zone</p> <p>Loaders are under active development and may change.</p>"},{"location":"guide/concepts/loaders_and_documents/#quickstart","title":"Quickstart","text":"<pre><code>import asyncio\nfrom marvin.loaders.web import SitemapLoader\n# loader that can parse text from all urls in a sitemap\nprefect_docs = SitemapLoader(\nurls=[\"https://docs.prefect.io/sitemap.xml\"],\nexclude=[\"api-ref\"],\n)\n# load, embed, store in Chroma locally at ~/.marvin/chroma/*.parquet\nasyncio.run(prefect_docs.load_and_store())\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/#_1","title":"Loaders","text":"<p>A <code>Loader</code> parses a source of information into a <code>list[Document]</code>, which can then be stored as context for the LLM.</p> <p></p>"},{"location":"guide/concepts/loaders_and_documents/#what-is-a-document","title":"What is a <code>Document</code>?","text":"<p>A <code>Document</code> is a rich Pydantic model that represents a store-able and searchable unit of information. </p> <p>A valid <code>Document</code> only requires one attribute, <code>text</code>: the raw text of the document.</p> <pre><code>from marvin.models.document import Document\ndocument = Document(text=\"This is a document.\")\n</code></pre> <p>You can attach arbitrary <code>Metadata</code> to a <code>Document</code>.</p> <pre><code>from marvin.models.documents import Document\nmy_document = Document(\ntext=\"This is a document.\",\nmetadata={\n\"title\": \"My Document\",\n\"link\": \"https://www.example.com\",\n\"random_metadata_field\": \"This is very important to me!\"\n}\n)\n</code></pre>"},{"location":"guide/concepts/loaders_and_documents/#creating-excerpts-from-a-document","title":"Creating excerpts from a <code>Document</code>","text":"<p><code>Document</code> offers a <code>to_excerpts</code> method that splits a <code>Document</code> into a <code>list[Document]</code> which contains rich excerpts of the original.</p> <p><pre><code># using the same document as above\nawait my_document.to_excerpts()\n# yields\n[\nDocument(\nid='doc_01GWMH5VA91P0SSYJXE9N7ZK88',\ntext='The following is a original document \\n# Document metadata\\n        Link: https://www.example.com\\nTitle: My Document\\nSource: unknown\\nDocument_type: original\\nRandom_metadata_field: This is very important to me!\\n# Excerpt content: This is a document.',\nembedding=None,\nmetadata=Metadata(\nlink='https://www.example.com',\ntitle='My Document',\nsource='unknown',\ndocument_type='excerpt',\nrandom_metadata_field='This is very important to me!'\n),\nsource=None,\ntype='excerpt',\nparent_document_id='doc_01GWMH58XYXFB8JQYC2T6VMFYC',\ntopic_name='marvin',\ntokens=55,\norder=0,\nkeywords=['document']\n)\n]\n</code></pre> Here, since our <code>Document</code> is short, there's only one excerpt. Longer documents are split into many excerpts according to the <code>chunk_tokens</code> argument of <code>to_excerpts</code>.</p> <p>You'll notice that the <code>Document</code>'s <code>text</code> attribute has been replaced with a rich excerpt that includes the original <code>Document</code>'s <code>Metadata</code> and the excerpt's location in the original <code>Document</code>. This replacement helps provide more context to the LLM when it's searching for answers.</p>"},{"location":"guide/concepts/loaders_and_documents/#how-can-i-create-my-own-loader","title":"How can I create my own <code>Loader</code>?","text":"<p>One way or another, a <code>Loader</code> must return a <code>list[Document]</code>. These <code>Document</code>s can be created in any way you like, but their <code>text</code> must have fewer tokens than the limit for your embedding function. </p> <p>For example, if you're using Marvin's default: OpenAI's <code>text-embedding-ada-002</code>, the limit is 8191 tokens.</p> <p>This is where <code>Document.to_excerpts</code> comes in handy. </p> <p>You can create a <code>Document</code> with a large <code>text</code> attribute, and split it into many <code>Document</code> excerpts to <code>extend</code> the <code>list[Document]</code> you're returning - the bonus being that you'll get rich excerpts as described above.</p>"},{"location":"guide/concepts/loaders_and_documents/#example-pokemonloader","title":"Example: <code>PokemonLoader</code>","text":"<p>For example, one could create a <code>PokemonLoader</code> that loads Pokemon data from the PokeAPI.</p> <pre><code>import httpx\nimport asyncio\nfrom marvin.loaders.base import Loader\nfrom marvin.models.documents import Document\nfrom marvin.models.metadata import Metadata\nasync def fetch_data(url: str) -&gt; dict:\nasync with httpx.AsyncClient() as client:\nreturn (await client.get(url)).json()\nasync def create_document(url: str) -&gt; Document:\npokemon_data = await fetch_data(url)\nspecies_data = await fetch_data(pokemon_data['species']['url'])\nflavor_text = next(\n(\nentry['flavor_text'].replace('\\n', ' ')\nfor entry in species_data['flavor_text_entries']\nif entry['language']['name'] == 'en'\n),\n\"\"\n)\nreturn Document(\ntext=f\"{pokemon_data['name'].capitalize()}: {flavor_text}\",\nmetadata=Metadata(\ntitle=pokemon_data['name'],\npokemon_type=pokemon_data['types'][0]['type']['name']\n)\n)\nclass PokemonLoader(Loader):\n\"\"\"Loads documents from the PokeAPI\"\"\"\nlimit: int = 5\nasync def load(self) -&gt; list[Document]:\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(f\"https://pokeapi.co/api/v2/pokemon?limit={self.limit}\")\ndata = response.json()\ndocuments = await asyncio.gather(\n*[create_document(pokemon['url']) for pokemon in data['results']]\n)\nexcerpts = []\nfor document in documents:\nexcerpts.extend(await document.to_excerpts())\nreturn excerpts\nif __name__ == \"__main__\":\nprint(asyncio.run(PokemonLoader().load()))\n</code></pre> <p>Note</p> <p>Like most of the code in Marvin, the <code>load</code> method must be <code>async</code>.</p> <p>Try it out! - Copy the above code into a Python script or <code>jupyter notebook</code>. - Run the script or cell.</p>"},{"location":"guide/concepts/mapping/","title":"\ud83e\udead Mapping","text":"<p>Sometimes you may want to apply an AI function or AI model over a whole iterable of items. For example, you may want to perform a sentiment analysis with an <code>ai_fn</code> over a list of tweets, or coerce a list of documents into a list of <code>ai_model</code>s.</p> <p>At scale it can be quite slow to process each item sequentially, so Marvin provides a <code>.map</code> method that allows you to process items concurrently, or in parallel - powered by Prefect.</p> <p>Tip</p> <p>The <code>.map</code> method was released in <code>marvin==0.9.2</code>.</p>"},{"location":"guide/concepts/mapping/#mapping-ai_fn-over-an-iterable","title":"Mapping <code>ai_fn</code> over an iterable","text":"<pre><code>from marvin import ai_fn\n@ai_fn(llm_model_name=\"gpt-3.5-turbo\")\ndef opposite(something: str) -&gt; str:\n\"\"\"Return the opposite of `something`\"\"\"\nopposites = opposite.map([\"up\", \"hot\", \"heads\"])\nprint(opposites)\n</code></pre> <p>Calling <code>.map</code> on an AI function will spin up a Prefect flow, with one task per item in the iterable. The tasks will be executed concurrently, and the results will be returned as a list: <pre><code>08:47:14.727 | INFO    | prefect.engine - Created flow run 'meaty-viper' for flow 'opposite'\n08:47:15.626 | INFO    | Flow run 'meaty-viper' - Created task run 'opposite-1' for task 'opposite'\n08:47:15.628 | INFO    | Flow run 'meaty-viper' - Submitted task run 'opposite-1' for execution.\n08:47:15.657 | INFO    | Flow run 'meaty-viper' - Created task run 'opposite-2' for task 'opposite'\n08:47:15.658 | INFO    | Flow run 'meaty-viper' - Submitted task run 'opposite-2' for execution.\n08:47:15.722 | INFO    | Flow run 'meaty-viper' - Created task run 'opposite-0' for task 'opposite'\n08:47:15.723 | INFO    | Flow run 'meaty-viper' - Submitted task run 'opposite-0' for execution.\n08:47:16.781 | INFO    | Task run 'opposite-2' - Finished in state Completed()\n08:47:16.790 | INFO    | Task run 'opposite-0' - Finished in state Completed()\n08:47:17.018 | INFO    | Task run 'opposite-1' - Finished in state Completed()\n08:47:17.170 | INFO    | Flow run 'meaty-viper' - Finished in state Completed('All states completed.')\n['down', 'cold', 'tails']\n</code></pre></p>"},{"location":"guide/concepts/mapping/#mapping-ai_model-over-an-iterable","title":"Mapping <code>ai_model</code> over an iterable","text":"<p>Similarly, you can call <code>.map</code> on an AI model to process each item in the iterable with the model. The results will be returned as a list of hydrated models:</p> <pre><code>from marvin import ai_model\n@ai_model(llm_model_name=\"gpt-3.5-turbo\")\nclass Location(BaseModel):\ncity: str\ncountry: str\nlatitute: float\nlongitude: float\nlocations = Location.map([\"windy city\", \"big apple\", \"mile high city\"])\nprint(locations)\n# [\n#   Location(city='Chicago',country='United States', latitute=41.8781, longitude=-87.6298),\n#   Location(city='New York', country='United States', latitute=40.7128, longitude=-74.006),\n#   Location(city='Denver', country='United States', latitute=39.7392, longitude=-104.9903)\n# ]\n</code></pre> <p>Tip</p> <p>Since mapping is powered by Prefect, you can run <code>prefect server start</code> to launch an observability UI for all your Marvin workflows. You can also use Marvin with Prefect Cloud's managed workflow platform.</p>"},{"location":"guide/concepts/mapping/#caching-results","title":"Caching results","text":"<p>Now that each ai function or ai model is being executed as a Prefect task, you can take advantage of Prefect's caching feature to avoid re-processing items / paying for duplicate AI calls.</p> <p>Let's add one import to our example above to enable caching: <pre><code>from prefect.tasks import task_input_hash\nfrom marvin import ai_fn\n@ai_fn(llm_model_name=\"gpt-3.5-turbo\")\ndef opposite(something: str) -&gt; str:\n\"\"\"Return the opposite of `something`\"\"\"\nopposites = opposite.map(\n[\"up\", \"hot\", \"heads\"],\ntask_kwargs=dict(cache_key_fn=task_input_hash)\n)\nprint(opposites)\n</code></pre> After the first execution, the results can be pulled from cache and subsequent executions will be much faster (and free \ud83d\ude42): <pre><code>09:08:46.428 | INFO    | prefect.engine - Created flow run 'fantastic-caterpillar' for flow 'opposite'\n09:08:47.116 | INFO    | Flow run 'fantastic-caterpillar' - Created task run 'opposite-2' for task 'opposite'\n09:08:47.118 | INFO    | Flow run 'fantastic-caterpillar' - Submitted task run 'opposite-2' for execution.\n09:08:47.134 | INFO    | Flow run 'fantastic-caterpillar' - Created task run 'opposite-1' for task 'opposite'\n09:08:47.135 | INFO    | Flow run 'fantastic-caterpillar' - Submitted task run 'opposite-1' for execution.\n09:08:47.156 | INFO    | Flow run 'fantastic-caterpillar' - Created task run 'opposite-0' for task 'opposite'\n09:08:47.156 | INFO    | Flow run 'fantastic-caterpillar' - Submitted task run 'opposite-0' for execution.\n09:08:47.349 | INFO    | Task run 'opposite-2' - Finished in state Cached(type=COMPLETED)\n09:08:47.372 | INFO    | Task run 'opposite-0' - Finished in state Cached(type=COMPLETED)\n09:08:47.375 | INFO    | Task run 'opposite-1' - Finished in state Cached(type=COMPLETED)\n09:08:47.496 | INFO    | Flow run 'fantastic-caterpillar' - Finished in state Completed('All states completed.')\n['down', 'cold', 'tails']\n</code></pre></p> <p>Tip</p> <p>See the docs on Prefect tasks to see all the different task configuration options! For example, you can achieve true parallelism with <code>task_runner=DaskTaskRunner()</code>, or you can use <code>retries=2</code> to retry failed tasks.</p>"},{"location":"guide/concepts/plugins/","title":"\ud83d\udd0c Plugins","text":"<p>Features</p> <p>\ud83e\uddb8 Give bots the ability to access new information and abilities</p> <p>\ud83e\uddbe Turn any function into a bot plugin</p> <p>Plugins extend a bot's functionality by letting it call a function and see the returned value. Plugins must be provided to a bot when it is instantiated, and the bot will decide whether to use a plugin based on its description. Users can influence that choice through instruction (e.g. telling the bot to use a specific plugin). </p>"},{"location":"guide/concepts/plugins/#writing-plugins","title":"Writing plugins","text":"<p>The simplest way to write a plugin is using the <code>@plugin</code> decorator. Note that plugin functions must have a docstring, as this is displayed to the bot so it can decide if it should use a plugin or not.</p> <pre><code>from marvin import Bot, plugin\nimport random\n@plugin\ndef random_number(min:float, max:float) -&gt; float:\n\"\"\"Use this plugin to generate a random number between min and max\"\"\"\nreturn min + (max - min) * random.random()\nbot = Bot(plugins=[random_number])\nawait bot.say('Use the plugin to pick a random number between 41 and 43')\n</code></pre> <p>For more complex plugins, you can inherit from the <code>marvin.Plugin</code> base class and implement a <code>run()</code> method. Class-based plugins must also have a <code>description</code> attribute. This is the equivalent of the function-based plugin above:</p> <pre><code>from marvin import Bot, Plugin\nimport random\nclass RandomNumber(Plugin):\ndescription: str = \"Use this plugin to generate a random number between min and max\"\ndef run(self, min:float, max:float) -&gt; float:\nreturn min + (max - min) * random.random()\nbot = Bot(plugins=[RandomNumber()])\nawait bot.say('Use the plugin to pick a random number between 41 and 43')\n</code></pre>"},{"location":"guide/concepts/plugins/#example-github-issue-search","title":"Example: GitHub Issue Search","text":"<p>To illustrate a real-world use case for plugins, we'll write one that searches for GitHub issues related to a specific topic.</p> <p>Psst!</p> <p>This plugin already exists at <code>marvin.plugins.github.search_github_issues</code>.</p> <p>All we really need to do is write a python function that accepts a <code>query</code> and <code>repo</code> argument, and returns a string summary of the most relevant issues. Remember that whatever arguments the function accepts, the bot will need to be able to provide. </p> <p>So, here's our function signature so far:</p> <pre><code>async def search_github_issues(\nquery: str,\nrepo: str = \"prefecthq/prefect\",\nn: int = 3\n) -&gt; str:\n</code></pre> <p>Luckily, the GitHub API makes the actual implementation easy. We'll use the <code>httpx</code> library to make an API call:</p> <pre><code>headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n# let's increase our rate limit by using an auth token if we have one\nif token := marvin.settings.github_token.get_secret_value():\nheaders[\"Authorization\"] = f\"Bearer {token}\"\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(\n\"https://api.github.com/search/issues\",\nheaders=headers,\nparams={\n\"q\": f\"repo:{repo} {query}\",\n\"order\": \"desc\",\n\"per_page\": n,\n},\n)\nresponse.raise_for_status()\n</code></pre> <p>The last thing we need to think about is how we want to present the results. We'll use a Pydantic model to parse our issues and then format them as a string:</p> <pre><code>issues_data = response.json()[\"items\"]\n# enforce 1000 token limit per body\nfor issue in issues_data:\nissue[\"body\"] = slice_tokens(issue[\"body\"], 1000)\nissues = [GitHubIssue(**issue) for issue in issues_data]\nreturn \"\\n\\n\".join(\nf\"{issue.title} ({issue.html_url}):\\n{issue.body}\" for issue in issues\n)\n</code></pre> <p>Note</p> <p>The <code>slice_tokens</code> function is a utility function that limits the number of tokens in a string. It's important to remember that our messages to the LLM must remain under the limit for the model we're using.</p>"},{"location":"guide/concepts/plugins/#all-together-now","title":"All together now","text":"<pre><code>import httpx\nimport marvin\nfrom marvin.loaders.github import GitHubIssue\nfrom marvin.plugins import plugin\nfrom marvin.utilities.strings import slice_tokens\n@plugin\nasync def search_github_issues(\nquery: str, repo: str = \"prefecthq/prefect\", n: int = 3\n) -&gt; str:\n\"\"\"\n    Use the GitHub API to search for issues in a given repository.\n    For example, to search for issues with the label \"bug\" in PrefectHQ/prefect:\n        - repo: prefecthq/prefect\n        - query: label:bug\n    \"\"\"\nheaders = {\"Accept\": \"application/vnd.github.v3+json\"}\nif token := marvin.settings.github_token.get_secret_value():\nheaders[\"Authorization\"] = f\"Bearer {token}\"\nasync with httpx.AsyncClient() as client:\nresponse = await client.get(\n\"https://api.github.com/search/issues\",\nheaders=headers,\nparams={\n\"q\": f\"repo:{repo} {query}\",\n\"order\": \"desc\",\n\"per_page\": n,\n},\n)\nresponse.raise_for_status()\nissues_data = response.json()[\"items\"]\nfor issue in issues_data:\nissue[\"body\"] = slice_tokens(issue[\"body\"], 1000)\nissues = [GitHubIssue(**issue) for issue in issues_data]\nreturn \"\\n\\n\".join(\nf\"{issue.title} ({issue.html_url}):\\n{issue.body}\" for issue in issues\n)\n</code></pre>"},{"location":"guide/concepts/plugins/#using-the-plugin","title":"Using the plugin","text":"<p>Now that we have it, we can use our plugin anywhere we have a bot. Let's try it out and search for issues in the <code>marvin</code> repository related to <code>nest_asyncio</code>.</p> <pre><code>from marvin.bot import Bot\nfrom marvin.plugins.github import search_github_issues\nbot = Bot(plugins=[search_github_issues])\nresponse = await bot.say('Any issues about nest_asyncio in PrefectHQ/marvin repo?')\nprint(response.content)\n</code></pre> <p></p>"},{"location":"guide/concepts/plugins/#technical-note-plugin-registration","title":"Technical note: plugin registration","text":"<p>Plugins that inherit from <code>marvin.Plugin</code> automatically register themselves for deserialization based on their class name. Bots are serialized with a reference to the plugin name and load the appropriate plugin upon deserialization. In a situation where you want to avoid conflict, you can manually set the deserialization key:</p> <pre><code>class MyPlugin(marvin.Plugin):\n_discriminator = 'my-key'\n</code></pre> <p>In order for a bot to use a plugin, the plugin must be available and imported prior to the plugin being deserialized. Otherwise it will not be properly registered when the bot is loaded. </p>"},{"location":"guide/concepts/tui/","title":"\ud83d\udda5\ufe0f TUI","text":"<p>Features</p> <p>\ud83d\udda5\ufe0f Full chat interface in your terminal</p> <p>\ud83e\udd16 Create bots with different personalities and objectives</p> <p>\ud83e\uddf5 Create multiple persistent threads</p> <p>Marvin includes a full-featured TUI, or text user interface, that runs entirely in your terminal. The TUI makes it easy to chat with your bots, as well as manage them, for quick non-programmatic interactions.</p>"},{"location":"guide/concepts/tui/#starting-the-tui","title":"Starting the TUI","text":"<p>To start the TUI, type <code>marvin chat</code> into your terminal. You should see this screen: </p> <p>Avoid terminal.app on MacOS</p> <p>On MacOS, the default terminal gives a poor TUI experience. You might prefer another terminal, like iTerm2 or Warp. See the Textual FAQ for more detail.</p>"},{"location":"guide/concepts/tui/#chatting","title":"Chatting","text":"<p>To chat with a bot, begin typing in the input at the bottom of the screen and hit enter. Your message will appear and Marvin will respond. You'll notice your converation appears in the left sidebar as a new thread. </p> <p>Responses are rendered as markdown. Unfortunately, you can't enter multi-line responses yet, but this is a highly-requested Textual feature.</p> <p></p>"},{"location":"guide/concepts/tui/#copying-messages","title":"Copying messages","text":"<p>To copy a message, enter the message editing mode by clicking on it. Then click the \"Copy\" button to copy the message contents to the clipboard. </p>"},{"location":"guide/concepts/tui/#deleting-messages","title":"Deleting messages","text":"<p>You can delete messages if you want to restart your conversation from a certain point. Note that deleting a message also deletes all subsequent messages. To delete a message, enter the message editing mode by clicking on it. Then click the \"Delete\" button. </p>"},{"location":"guide/concepts/tui/#threads","title":"Threads","text":"<p>Each conversation in Marvin is called a \"thread\". Threads are persistent and can have potentially many participants (both bot and human). In the TUI, each thread is a distinct history of messages between you and a bot. </p> <p>Threads are shown in the sidebar on the left side of the screen. You will only see threads that include the bot you're currently talking to.</p>"},{"location":"guide/concepts/tui/#deleting-threads","title":"Deleting threads","text":"<p>To delete a thread, select it from the left hand sidebar and press the \"Delete thread\" button.</p>"},{"location":"guide/concepts/tui/#auto-naming-threads","title":"Auto-naming threads","text":"<p>Marvin does its best to name the threads, taking the message history into account as well as the bot's personality. It will update the thread name until the bot has responded more than five times.</p>"},{"location":"guide/concepts/tui/#bots","title":"Bots","text":"<p>Bots are what set Marvin apart from a simple chat interface. Marvin bots all have personalities and instructions, so you can save bots that are particularly useful to you. You might have one bot that only writes formal emails, another that brainstorms engineering solutions, a third that plays role-playing games, and another that only creates memes. It's completely up to you.</p>"},{"location":"guide/concepts/tui/#changing-bots","title":"Changing bots","text":"<p>To speak to a different bot, press the \"Bots\" button and choose a different bot from the menu. </p>"},{"location":"guide/concepts/tui/#changing-the-default-bot","title":"Changing the default bot","text":"<p>By default, Marvin is selected as the active bot when the TUI starts. To change this, provide a different bot's name when starting the TUI. If the provided name is not found, it will be ignored.</p> <pre><code>marvin chat -b AnotherBot\n</code></pre>"},{"location":"guide/concepts/tui/#creating-and-updating-bots","title":"Creating and updating bots","text":"<p>You can talk to the default Marvin bot to create other bots - just describe the bot you want and, once it collects enough information, it can use plugins to create the bot for you. You can also explore and update existing bots this way. </p> <p>If you prefer a programmatic interface, you can create bots either in Python:</p> <pre><code>from marvin import Bot\nbot = Bot(\nname=\"MyBot\",\ndescription=\"A description\",\npersonality=\"A personality\",\ninstructions=\"Some instructions\",\n)\nbot.save_sync()\n# to update an existing bot with the same name\nbot.save_sync(if_exists='update')\n</code></pre> <p>Or from the CLI:</p> <pre><code>marvin bot create MyBot -d \"A description\" -p \"A personality\" -i \"Some instructions\"\n</code></pre> <p>You can also edit and delete bots the same way.</p>"},{"location":"guide/concepts/tui/#settings","title":"Settings","text":"<p>You can set your OpenAI API key from the TUI by pressing the Settings button. It will be validated and stored in your Marvin config for future sessions, including interactive use outside the TUI.</p> <p></p>"},{"location":"guide/concepts/tui/#upgrading-the-database","title":"Upgrading the database","text":"<p>When new versions of Marvin are released, they may require your database to be upgraded in order to handle the new features or enhancements they contain. Therefore, whenever Marvin starts, it checks to see if your database is up-to-date and prints a helpful warning if it isn't. The TUI can go a step farther and upgrade your database automatically. If it is possible to do so, you will see a screen like the one below.</p> <p></p> <p>Manually upgrading the database</p> <p>We have tried to prevent the TUI from accessing the database until after it runs the upgrade check and shows the warning screen. However, if for some reason it is unable to run the check properly, it may crash. In this case you can manually upgrade the database by running <code>marvin database upgrade</code> in your terminal.</p>"},{"location":"guide/concepts/tui/#technology","title":"Technology","text":"<p>The Marvin TUI is built with Textual, a Python library for building TUIs.</p>"},{"location":"guide/introduction/cli/","title":"CLI","text":"<p>See all available commands:</p> <pre><code>marvin --help\n</code></pre>"},{"location":"guide/introduction/cli/#tui","title":"TUI","text":"<p>To launch the TUI: <pre><code>marvin chat\n</code></pre></p>"},{"location":"guide/introduction/cli/#bot-management","title":"Bot management","text":"<p>To create a new bot: <pre><code>marvin bots create -n MyBot -d \"a description of the bot\" -p \"a personality\"\n</code></pre></p> <p>To list all bots: <pre><code>marvin bots ls\n</code></pre></p> <p>To update a bot: <pre><code>marvin bots update MyBot -p \"a new personality\"\n</code></pre></p> <p>To delete a bot: <pre><code>marvin bots delete MyBot\n</code></pre></p>"},{"location":"guide/introduction/cli/#database-management","title":"Database management","text":"<pre><code>marvin database --help\n</code></pre> <p>WARNING: This will drop all tables and data in the database. <pre><code>marvin database reset\n</code></pre></p>"},{"location":"guide/introduction/cli/#run-the-server","title":"Run the server","text":"<pre><code>marvin server start\n</code></pre>"},{"location":"guide/introduction/cli/#setup-openai","title":"Setup OpenAI","text":"<pre><code>marvin setup-openai\n</code></pre>"},{"location":"guide/introduction/configuration/","title":"Configuration","text":""},{"location":"guide/introduction/configuration/#llms","title":"LLMs","text":""},{"location":"guide/introduction/configuration/#selecting-an-llm-backend","title":"Selecting an LLM backend","text":"<p>Marvin supports various LLM backends. The default is OpenAI's GPT-3.5 model, but you can choose from a number of different providers, including OpenAI, Anthropic, Azure, HuggingFaceHub, and more. To change the LLM model, set the <code>MARVIN_LLM_BACKEND</code> and <code>MARVIN_LLM_MODEL</code> variables appropriately. For some common models, the backend can be selected automatically (e.g. any GPT model, any Claude model).</p> <p>At this time, valid options include:</p> Backend Description Models Notes <code>OpenAIChat</code> (default) OpenAI's chat models <code>gpt-3.5-turbo</code> (default), <code>gpt-4</code>, etc. Marvin is generally tested and optimized with this backend. <code>AzureOpenAIChat</code> OpenAI chat models via Azure (same as <code>OpenAIChat</code>) <code>OpenAI</code> OpenAI's completion models <code>text-davinci-003</code>, etc. <code>AzureOpenAI</code> OpenAI completion models via Azure (same as <code>OpenAI</code>) <code>Anthropic</code> Anthropic models <code>claude-v1</code>, <code>claude-v1.3</code>, <code>claude-v1.3-100k</code>, any other available model <code>HuggingFaceHub</code> Models hosted on HuggingFaceHub Any valid <code>repo/model</code> combination Support for these models is experimental and quality will vary. <p>Models may have unique settings which are detailed below.</p>"},{"location":"guide/introduction/configuration/#openai","title":"OpenAI","text":"<p>Marvin can use the OpenAI API to access state-of-the-art language models, including GPT-4 and GPT-3.5. These models are capable of generating natural language text that is often indistinguishable from human-written text. By integrating with the OpenAI API, Marvin provides a simple interface for accessing these powerful language models and incorporating them into your own applications.</p> <p>Note that Marvin uses OpenAI's gpt-3.5-turbo model by default, but you must provide an API key for this to work.</p>"},{"location":"guide/introduction/configuration/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your an OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"guide/introduction/configuration/#setting-the-api-key-via-cli","title":"Setting the API key via CLI","text":"<p>The easiest way to set your API key is by running <code>marvin setup-openai</code>, which will let you store your API key in your Marvin configuration file. </p>"},{"location":"guide/introduction/configuration/#environment-variables","title":"Environment variables","text":"<p>Alternatively, you can provide your API key as an environment variable (as with any Marvin setting). Marvin will check <code>MARVIN_OPENAI_API_KEY</code> followed by <code>OPENAI_API_KEY</code>. The latter is more standard and may be accessed by multiple libraries, but the former can be used to scope the API key for Marvin's use only. These docs will use <code>MARVIN_OPENAI_API_KEY</code> but either will work.</p> <p>To set your OpenAI API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p>"},{"location":"guide/introduction/configuration/#database","title":"Database","text":"<p>Running Marvin as an API server requires a database. By default, Marvin uses a SQLite database stored at <code>~/.marvin/marvin.sqlite</code>. You can set the database location and type by changing the <code>MARVIN_DATABASE_CONNECTION_URL</code> setting. Marvin is tested with SQLite and Postgres. It may also work with other database supported by SQLAlchemy, so long as there are async drivers available.</p> <p>Warning</p> <p>Marvin's server is under active development, so you should treat its database as ephemeral and able to be destroyed at any time. At this time, Marvin does not include database migrations, which means that upgrading your database schema requires destroying it. This is a high-priority area for improvement.</p>"},{"location":"guide/introduction/configuration/#settings","title":"Settings","text":"<p>Marvin has many configurable settings that can be loaded from <code>marvin.settings</code>.</p>"},{"location":"guide/introduction/configuration/#setting-values","title":"Setting values","text":"<p>All settings can be configured via environment variable using the pattern <code>MARVIN_&lt;name of setting&gt;</code>. For example, to set the log level, set <code>MARVIN_LOG_LEVEL=DEBUG</code> and verify that <code>marvin.settings.log_level == 'DEBUG'</code>. Settings can also be set at runtime through assignment (e.g. <code>marvin.settings.log_level = 'DEBUG'</code>) but this is not recommended because some code might haved loaded configuration on import and consequently never pick up the updated value.</p>"},{"location":"guide/introduction/configuration/#important-settings","title":"Important settings","text":""},{"location":"guide/introduction/configuration/#global","title":"Global","text":"<p>Log level: Set the log level <pre><code>MARVIN_LOG_LEVEL=INFO\n</code></pre> Verbose mode: Logs extra information, especially when the log level is <code>DEBUG</code>.  <pre><code>MARVIN_VERBOSE=true\n</code></pre></p>"},{"location":"guide/introduction/configuration/#openai_1","title":"OpenAI","text":"<p>API key: Set your OpenAI API key <pre><code>MARVIN_OPENAI_API_KEY=\n</code></pre> Marvin will also respect this global variable <pre><code>OPENAI_API_KEY=\n</code></pre></p> <p>Model name:  Choose the LLM model. <pre><code>MARVIN_LLM_MODEL='gpt-4'\n</code></pre></p>"},{"location":"guide/introduction/configuration/#database_1","title":"Database","text":"<p>Database connection URL: Set the database connection URL. Must be a fully-qualified URL. Marvin supports both Postgres and SQLite.</p> <pre><code>MARVIN_DATABASE_CONNECTION_URL=\n</code></pre>"},{"location":"guide/introduction/overview/","title":"The Guide","text":"<p>Welcome to the ever-expanding guide to Marvin.  The documentation is split into a few key sections:</p> <ul> <li>Introduction: information about configuration and basic use</li> <li>Concepts: introductions, explanations, and examples of the most important Marvin concepts</li> <li>AI Functions Library: an overview of the functionality included in Marvin's builtin AI functions</li> <li>Use Cases: common use cases and examples to help you get started</li> <li>Development: details on developing the Marvin library itself</li> </ul>"},{"location":"guide/use_cases/enforcing_format/","title":"Enforcing LLM output formats","text":"<p>One of the most important \"unlocks\" for using AIs alongside and within your code is working with native data structures. This can be challenging for two reasons: first, because LLMs naturally exchange information through unstructured text; and second, because modern LLMs are trained with conversational objectives, so they have a tendency to interject extra words like \"Sure, here's the data you requested:\". This makes extracting structured outputs difficult.</p> <p>Marvin can be used to get AIs to respond in structured, parseable forms. There are two common ways to enable this functionality, depending on whether you're using AI functions or bots. With AI functions, provide a return type annotation. With bots, provide a <code>response_format</code> argument. </p> <p>Under the hood, Marvin is creating a <code>ResponseFormatter</code> object that can handle sending instructions to the AI, validating the response, and parsing the output. It can even take steps to fix invalid responses. Most users will never have to create <code>ResponseFormatters</code> by hand, as Marvin will usually \"do the right thing\" when a return annotation or <code>response_format</code> is provided. </p>"},{"location":"guide/use_cases/enforcing_format/#learn-more","title":"Learn more","text":"<p>For more detail, see the bots docs.</p>"},{"location":"guide/use_cases/enforcing_format/#examples","title":"Examples","text":"<p>Examples are shown for both AI functions and bots.</p>"},{"location":"guide/use_cases/enforcing_format/#returning-a-string","title":"Returning a string","text":"<p><pre><code>@ai_fn\ndef my_fn() -&gt; str:\n\"\"\"This function will return a string\"\"\"\n</code></pre> <pre><code>Bot() # bots return strings by default\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#returning-a-list-of-dicts","title":"Returning a list of dicts","text":"<p><pre><code>@ai_fn\ndef my_fn() -&gt; list[dict]:\npass\n</code></pre> <pre><code>Bot(response_format=list[dict])\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#pydantic-models","title":"Pydantic models","text":"<p><pre><code>class MyOutput(pydantic.BaseModel):\nx: int\ny: list[dict]\n@ai_fn\ndef my_fn() -&gt; list[MyOutput]:\n\"\"\"This function will return a list of MyOutput models\"\"\"\n</code></pre> <pre><code>Bot(response_format=MyOutput)\n</code></pre></p>"},{"location":"guide/use_cases/enforcing_format/#json-objects","title":"JSON objects","text":"<p>Instead of using Python types, you can describe the shape of the output. If your description includes the word \"json\", it will be automatically parsed and validated; otherwise it will be returned as a string</p> <p>For example, these will both return structured objects: <pre><code>@ai_fn\ndef my_fn() -&gt; 'a JSON list of strings and ints':\n\"\"\"This function will return list[str | int]\"\"\"\n</code></pre> <pre><code>Bot(response_format = 'a JSON list of strings and ints')\n</code></pre></p> <p>While these will return strings (that could be parsed with <code>json.loads()</code>). Note the absence of the word JSON, which is what hints to Marvin to add a JSON parser to the <code>ResponseFormatter</code>.</p> <p><pre><code>@ai_fn\ndef my_fn() -&gt; 'a list of strings and ints':\n\"\"\"This function will return list[str | int]\"\"\"\n</code></pre> <pre><code>Bot(response_format = 'a list of strings and ints')\n</code></pre></p>"},{"location":"guide/use_cases/slackbot/","title":"Running a Marvin-powered Slackbot","text":"<p>Note</p> <p>This guide assumes you have already created a Slack app and have a bot user. If you haven't created an app, you can follow the Slack documentation. You'll need an app with <code>app_mentions:read</code> and <code>chat:write</code> permissions.</p> <p>Marvin ships with endpoints supporting a customizable Slackbot that runs directly within the <code>marvin</code> FastAPI application.</p>"},{"location":"guide/use_cases/slackbot/#configuring-a-simple-local-slackbot","title":"Configuring a simple, local Slackbot","text":""},{"location":"guide/use_cases/slackbot/#create-a-bot","title":"Create a bot","text":"<p>Create any Marvin bot and save it:</p> <pre><code>import marvin\nbot = marvin.Bot(\nname=\"Suspiciously Nice Bot\",\npersonality=\"friendly... too friendly\"\n)\nbot.save_sync()\n</code></pre>"},{"location":"guide/use_cases/slackbot/#environment-variables","title":"Environment variables","text":"<p>Marvin will discover these settings whether you set them in a project <code>.env</code> file or in your shell config, let's set: <pre><code>MARVIN_OPENAI_API_KEY=&lt;your openai api key&gt;\nMARVIN_SLACK_API_TOKEN=&lt;your slack api token&gt;\nMARVIN_SLACK_BOT_NAME=&lt;your bot name (\"Suspiciously Nice Bot\" in this example)&gt;\nMARVIN_SLACK_BOT_ADMIN_USER=@&lt;your slack user id&gt;\nMARVIN_LOG_LEVEL=DEBUG\n</code></pre> and that's it! We can now use something like <code>ngrok</code> to get ourselves a public IP to hit from Slack:</p> <p><pre><code>ngrok http 4200\n</code></pre> We can grab the <code>ngrok</code> public URL being forwarded</p> <pre><code>Forwarding                    https://1303-24-1-189-9.ngrok-free.app -&gt; http://localhost:4200\n</code></pre> <p>and use it to set up our Slack app's \"Event Subscriptions\" to point to our bot's <code>/slack/events</code> endpoint:</p> <p></p> <p>... and then run our bot (running on port 4200 by default):</p> <pre><code>marvin server start\n</code></pre> <p>... and that's it! We can now mention our bot in Slack and it will respond according to our bot setup.</p> <p></p>"},{"location":"guide/use_cases/slackbot/#qaing-slackbot-responses-and-providing-feedback","title":"QA'ing Slackbot responses and providing feedback","text":"<p>You can enable a Slack-native feedback mechanism by setting the following environment variables:</p>"},{"location":"guide/use_cases/slackbot/#environment-variables_1","title":"Environment variables","text":"<pre><code>MARVIN_QA_SLACK_BOT_RESPONSES=true\nMARVIN_SLACK_BOT_QA_CHANNEL=&lt;your slack QA channel id&gt; # e.g. C01UJ9ZQZ0K\n</code></pre>"},{"location":"guide/use_cases/slackbot/#configuring-the-slack-app","title":"Configuring the Slack app","text":"<p>To use the feedback mechanism, we'll need to configure a <code>Request URL</code> in the <code>Interactivity &amp; Shortcuts</code> section of our Slack app.</p> <p>Note that these events are handled by the <code>/slack/block_actions</code> endpoint:</p> <p></p>"},{"location":"guide/use_cases/slackbot/#example","title":"Example","text":"<p>Now, whenever a Slack user tags the bot in a message</p> <p></p> <p>... a QA message will be sent to the configured QA channel</p> <p></p> <p>... in addition to the bot's response to the user's original message</p> <p></p> <p>So we can approve the response to do nothing, or click <code>Edit Response</code> provide a response for the bot to use in the future:</p> <p></p> <p>Once edited, we can <code>Discard</code> to do nothing or <code>Save Response to Chroma</code>:</p> <p></p> <p>... to add the question-answer pair as a <code>Document</code> to our active Chroma vectorstore:</p> <p></p> <p>Now, the bot will be able to use the <code>chroma_search</code> to retrieve this document in the future.</p>"},{"location":"guide/use_cases/slackbot/#deploying-a-slackbot-on-cloud-run","title":"Deploying a Slackbot on Cloud Run","text":"<p><code>ngrok</code> is great for testing, but it's not a great solution for a public-facing bot. For that, we'll need to deploy our bot somewhere with a public IP. For this example, we'll use Google Cloud Run.</p>"},{"location":"guide/use_cases/slackbot/#make-a-dockerfile","title":"Make a Dockerfile","text":"<p>We'll need to make a Dockerfile that installs Marvin and our bot's dependencies. </p> <p>We'll also need to run the <code>marvin database upgrade</code> command to initialize our SQLite database that stores our bot's state, like conversation history and bot configuration.</p> <pre><code>FROM prefecthq/prefect:2-python3.10\nWORKDIR /app\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN pip install \"marvin[chromadb]\"\nRUN pip uninstall uvloop -y\n\nRUN marvin database upgrade -y\n\nCOPY setup.py /app/setup.py\n\nCOPY entrypoint.sh /app/entrypoint.sh\n\nENTRYPOINT [\"/app/entrypoint.sh\"]\n</code></pre> <p>Note that we're copying in our <code>setup.py</code> file, which configures the <code>Bot</code> with its <code>plugins</code> and <code>instructions</code>.</p> <p>The <code>entrypoint.sh</code> file is a bash script that runs our <code>setup.py</code> file and then starts the <code>marvin</code> server:</p> <pre><code>#!/bin/sh\npython /app/setup.py\n\nexec uvicorn marvin.server:app --host 0.0.0.0 --port 4200\n</code></pre>"},{"location":"guide/use_cases/slackbot/#build-and-push-the-image-with-a-github-action","title":"Build and Push the image with a GitHub Action","text":"<p>Note: all of the following GitHub Action workflows use GCP workload identity federation to authenticate to google cloud &amp; perform actions. You can instead use regular auth by passing a GCP service account key to your workflows.</p> <p>Here's how we can build our slackbot image and push it to GCP Artifact Registry using a GitHub Action: <pre><code>---\nname: Build and publish slackbot image\non:\npush:\nbranches:\n- main\n# Do not grant jobs any permissions by default\npermissions: {}\njobs:\nbuild_push_image:\nname: Build slackbot image\nruns-on: ubuntu-latest\npermissions:\n# required to initiate a downstream workflow (read needed for codeql scan)\nactions: write\n# required to read from the repo\ncontents: read\n# required to obtain Google Cloud service account credentials\nid-token: write\nsteps:\n- name: Checkout repository\nuses: actions/checkout@v3\n- name: Authenticate to Google Cloud\nuses: google-github-actions/auth@v1\nwith:\nworkload_identity_provider: projects/GCP_PROJECT_NUMBER/locations/global/workloadIdentityPools/WORKLOAD_IDENTITY_POOL/providers/PROVIDER\nservice_account: SERVICE_ACCOUNT_NAME@GCP_PROJECT_NAME.iam.gserviceaccount.com\n- name: Configure Google Cloud credential helper\nrun: gcloud auth configure-docker --quiet us-docker.pkg.dev\n- name: Get image version\nrun: |\nshort_sha=$(git rev-parse --short=7 HEAD)\necho \"short_sha: ${short_sha}\"\necho \"SHORT_SHA=${short_sha}\" &gt;&gt; $GITHUB_ENV\n- name: Build container image\nrun: |\ndocker build ./path/to/dockerfile \\\n--no-cache \\\n--tag us-docker.pkg.dev/GCP_PROJECT_NAME/REGISTRY/slackbot:latest \\\n--tag us-docker.pkg.dev/GCP_PROJECT_NAME/REGISTRY/slackbot:${SHORT_SHA} \\\n- name: Push image to GCP project\nrun: docker push --all-tags us-docker.pkg.dev/GCP_PROJECT_NAME/REGISTRY/slackbot\n# optionally automatically deploy the latest revision to cloudrun (see below)\n- name: Trigger cloudrun revision deploy workflow\nrun: |\ngh workflow run deploy-cloudrun-slackbot.yaml \\\n--ref main\nenv:\nGITHUB_TOKEN: ${{ github.token }}\n</code></pre></p>"},{"location":"guide/use_cases/slackbot/#deploy-the-cloud-run-service-with-a-github-action","title":"Deploy the Cloud Run Service with a GitHub Action","text":"<p>Here's how we can deploy our bot to Cloud Run using a GitHub Action: <pre><code>---\nname: Deploy new revision of slackbot cloudrun service\non:\nworkflow_dispatch: {}\n# Do not grant jobs any permissions by default\npermissions: {}\njobs:\ndeploy_cloudrun_revision:\nname: Deploy revision with latest image\nruns-on: ubuntu-latest\npermissions:\n# required to read from the repo\ncontents: read\n# required to obtain Google Cloud service account credentials\nid-token: write\nsteps:\n- name: Checkout repository\nuses: actions/checkout@v3\n- name: Authenticate to google cloud\nuses: google-github-actions/auth@v1\nwith:\nworkload_identity_provider: projects/GCP_PROJECT_NUMBER/locations/global/workloadIdentityPools/WORKLOAD_IDENTITY_POOL/providers/PROVIDER\nservice_account: SERVICE_ACCOUNT_NAME@GCP_PROJECT_NAME.iam.gserviceaccount.com\n- name: Deploy revision\nuses: google-github-actions/deploy-cloudrun@v1\nwith:\nimage: us-docker.pkg.dev/GCP_PROJECT_NAME/REGISTRY/slackbot:latest\nproject_id: GCP_PROJECT_NAME\nregion: REGION\nservice: slackbot\n</code></pre></p>"},{"location":"guide/use_cases/slackbot/#deploy-the-cloud-run-service-infrastructure-with-terraform","title":"Deploy the Cloud Run Service [infrastructure] with Terraform","text":"<p>You can find sample terraform code under <code>docs/guide/use_cases/terraform</code> that will enable you to provision the Cloud Run Service using Infrastructure as Code. Update the <code>vars.tfvars</code> file to reflect your specific configuration and follow the below steps to setup your Cloud Run Service. <pre><code>cd ./docs/guide/use_cases/terraform\nterraform init\n# if granting the cloud run service access to pull from secrets manager, run this command first, otherwise skip:\nterraform apply -target google_service_account.cloudrun -target google_project_iam_member.cloudrun_secret_manager_accessor -var-file vars.tfvars terraform apply -var-file vars.tfvars\n</code></pre></p> <p>Note</p> <p>For more details on using Cloud Run, see the Cloud Run guide.</p>"}]}